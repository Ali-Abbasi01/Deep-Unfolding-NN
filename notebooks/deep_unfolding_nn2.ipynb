{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ec16ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Get the current working directory\n",
    "scripts_dir = os.getcwd()\n",
    "# Go up one level\n",
    "project_root = os.path.abspath(os.path.join(scripts_dir, '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import src.sc_wmmse\n",
    "importlib.reload(src.sc_wmmse)\n",
    "from src.sc_wmmse import WMMSE_alg_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e5917",
   "metadata": {},
   "source": [
    "Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1810c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the deep-unfolding NN architecture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the layer\n",
    "class Layer1(nn.Module):\n",
    "    def __init__(self, setup, flagg):\n",
    "        super().__init__()\n",
    "        self.setup = setup\n",
    "        X_U_dict = {}\n",
    "        Y_U_dict = {}\n",
    "        Z_U_dict = {}\n",
    "        O_U_dict = {}\n",
    "        X_W_dict = {}\n",
    "        Y_W_dict = {}\n",
    "        Z_W_dict = {}\n",
    "        X_V_dict = {}\n",
    "        Y_V_dict = {}\n",
    "        Z_V_dict = {}\n",
    "        O_V_dict = {}\n",
    "        for i in range(self.setup.K):\n",
    "            # X_U_dict[str(i)] = nn.Parameter(m01)\n",
    "            # Y_U_dict[str(i)] = nn.Parameter(m02)\n",
    "            # Z_U_dict[str(i)] = nn.Parameter(m03)\n",
    "            # O_U_dict[str(i)] = nn.Parameter(m04)\n",
    "            # X_W_dict[str(i)] = nn.Parameter(m05)\n",
    "            # Y_W_dict[str(i)] = nn.Parameter(m06)\n",
    "            # Z_W_dict[str(i)] = nn.Parameter(m07)\n",
    "            # X_V_dict[str(i)] = nn.Parameter(m08)\n",
    "            # Y_V_dict[str(i)] = nn.Parameter(m09)\n",
    "            # Z_V_dict[str(i)] = nn.Parameter(m010)\n",
    "            # O_V_dict[str(i)] = nn.Parameter(m011)\n",
    "            X_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "            Y_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "            Z_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "            O_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "            X_W_dict[str(i)] = nn.Parameter(torch.randn(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "            Y_W_dict[str(i)] = nn.Parameter(torch.randn(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "            Z_W_dict[str(i)] = nn.Parameter(torch.randn(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "            X_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "            Y_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "            Z_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "            O_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.d[i], dtype=torch.cdouble))\n",
    "            # X_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "            # Y_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "            # Z_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "            # O_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "            # X_W_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "            # Y_W_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "            # Z_W_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "            # X_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "            # Y_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "            # Z_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "            # O_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.d[i], dtype=torch.cdouble))\n",
    "        self.X_U = nn.ParameterDict(X_U_dict)\n",
    "        self.Y_U = nn.ParameterDict(Y_U_dict)\n",
    "        self.Z_U = nn.ParameterDict(Z_U_dict)\n",
    "        self.O_U = nn.ParameterDict(O_U_dict)\n",
    "        self.X_W = nn.ParameterDict(X_W_dict)\n",
    "        self.Y_W = nn.ParameterDict(Y_W_dict)\n",
    "        self.Z_W = nn.ParameterDict(Z_W_dict)\n",
    "        self.X_V = nn.ParameterDict(X_V_dict)\n",
    "        self.Y_V = nn.ParameterDict(Y_V_dict)\n",
    "        self.Z_V = nn.ParameterDict(Z_V_dict)\n",
    "        self.O_V = nn.ParameterDict(O_V_dict)\n",
    "\n",
    "\n",
    "    def forward(self, V, H):\n",
    "\n",
    "        # A^+ operator\n",
    "        # def plus(A):\n",
    "        #     diag_inv = 1.0 / A.diagonal(dim1=-2, dim2=-1)\n",
    "        #     A_plus = torch.zeros_like(A)\n",
    "        #     A_plus.diagonal(dim1=-2, dim2=-1).copy_(diag_inv)\n",
    "        #     return A_plus\n",
    "        # def plus(A: torch.Tensor) -> torch.Tensor:\n",
    "        #     \"\"\"\n",
    "        #     Moore-Penrose pseudo-inverse of a diagonal matrix: keeps autograd path.\n",
    "        #     Works batch-wise for ...×N×N tensors.\n",
    "        #     \"\"\"\n",
    "        #     diag = torch.diagonal(A, 0, -2, -1)          # keeps grad wrt A\n",
    "        #     inv  = 1.0 / diag                            # element-wise inverse\n",
    "        #     return torch.diag_embed(inv)\n",
    "        def plus(A):\n",
    "            return A\n",
    "\n",
    "        def proj_power(V):\n",
    "            V_proj = {}\n",
    "            for i in range(num_samples):\n",
    "                V_proj[str(i)] = {}\n",
    "                s = 0\n",
    "                for j in range(self.setup.K):\n",
    "                    s += torch.trace(V[str(i)][str(j)] @ V[str(i)][str(j)].conj().T)\n",
    "                for j in range(self.setup.K):\n",
    "                    V_proj[str(i)][str(j)] = torch.sqrt(self.setup.PT/s) * V[str(i)][str(j)]\n",
    "            return V_proj\n",
    "\n",
    "        # def proj_power(V_in: dict) -> dict:\n",
    "        #     \"\"\"\n",
    "        #     Return a *new* dictionary whose entries are power-normalised copies\n",
    "        #     of the Tx precoders in V_in.  No in-place writes → autograd path stays\n",
    "        #     intact when multiple unfolded layers are stacked.\n",
    "\n",
    "        #     V_in structure (unchanged):  {sample: {user: V_k}}\n",
    "        #     \"\"\"\n",
    "        #     V_out = {}\n",
    "        #     for sample, users in V_in.items():\n",
    "\n",
    "        #         # total power  (trace is complex → take real part)\n",
    "        #         P_tot = sum(torch.trace(v @ v.conj().T).real for v in users.values())\n",
    "        #         scale = torch.sqrt(self.setup.PT / (P_tot + 1e-12))     # +ε avoids div-by-0\n",
    "\n",
    "        #         V_out[sample] = {user: scale * v             # *clone* not needed\n",
    "        #                         for user, v in users.items()}\n",
    "        #     return V_out\n",
    "\n",
    "        # def proj_power_soft(V_in: dict, PT: float, alpha: float = 0.1) -> dict:\n",
    "        #     V_out = {}\n",
    "        #     for i, users in V_in.items():\n",
    "        #         P_tot = sum(torch.trace(v @ v.conj().T).real for v in users.values())\n",
    "        #         scale = torch.sqrt(PT / (P_tot + 1e-12))\n",
    "        #         V_out[str(i)] = {\n",
    "        #             str(k): (1 - alpha) * v + alpha * scale * v for k, v in users.items()\n",
    "        #         }\n",
    "        #     return V_out\n",
    "\n",
    "\n",
    "        num_samples = len(H)\n",
    "\n",
    "        # Calculate A\n",
    "        A = {}\n",
    "        for i in range(num_samples):\n",
    "            A[str(i)] = {}\n",
    "            for j in range(self.setup.K):\n",
    "                s = 0\n",
    "                for k in range(self.setup.K):\n",
    "                    s += torch.trace(V[str(i)][str(k)].conj().T @ V[str(i)][str(k)])\n",
    "                ey = (1/self.setup.PT) * s * torch.eye(self.setup.n_rx[j], dtype=torch.cdouble)\n",
    "                s = 0\n",
    "                for k in range(self.setup.K):\n",
    "                    s += V[str(i)][str(k)] @ V[str(i)][str(k)].conj().T\n",
    "                A[str(i)][str(j)] = ey + H.iloc[i, j] @ s @ H.iloc[i, j].conj().T\n",
    "\n",
    "        # Calculate U\n",
    "        U = {}\n",
    "        for i in range(num_samples):\n",
    "            U[str(i)] = {}\n",
    "            for j in range(self.setup.K):\n",
    "                A_inv = plus(A[str(i)][str(j)]) @ self.X_U[str(j)].to(torch.cdouble) + A[str(i)][str(j)] @ self.Y_U[str(j)].to(torch.cdouble) + self.Z_U[str(j)].to(torch.cdouble)\n",
    "                U[str(i)][str(j)] = A_inv @ H.iloc[i, j] @ V[str(i)][str(j)] + self.O_U[str(j)].to(torch.cdouble)\n",
    "\n",
    "        # Calclate E\n",
    "        E = {}\n",
    "        for i in range(num_samples):\n",
    "            E[str(i)] = {}\n",
    "            for j in range(self.setup.K):\n",
    "                E[str(i)][str(j)] = torch.eye(self.setup.d[j], dtype=torch.cdouble) - U[str(i)][str(j)].conj().T @ H.iloc[i, j] @ V[str(i)][str(j)]\n",
    "        \n",
    "        # Calculate W\n",
    "        W = {}\n",
    "        for i in range(num_samples):\n",
    "            W[str(i)] = {}\n",
    "            for j in range(self.setup.K):\n",
    "                W[str(i)][str(j)] = plus(E[str(i)][str(j)]) @ self.X_W[str(j)].to(torch.cdouble) + E[str(i)][str(j)] @ self.Y_W[str(j)].to(torch.cdouble) + self.Z_W[str(j)].to(torch.cdouble)\n",
    "\n",
    "        # Calculate B\n",
    "        B = {}\n",
    "        for i in range(num_samples):\n",
    "            s = 0\n",
    "            for k in range(self.setup.K):\n",
    "                s += torch.trace(U[str(i)][str(k)] @ W[str(i)][str(k)] @ U[str(i)][str(k)].conj().T)\n",
    "            ey = (1/self.setup.PT) * s * torch.eye(self.setup.n_tx, dtype=torch.cdouble)\n",
    "            s = 0\n",
    "            for k in range(self.setup.K):\n",
    "                s += H.iloc[i, k].conj().T @ U[str(i)][str(k)] @ W[str(i)][str(k)] @ U[str(i)][str(k)].conj().T @ H.iloc[i, k]\n",
    "            B[str(i)] = ey + s\n",
    "                \n",
    "\n",
    "        # Calculate V\n",
    "        V = {}\n",
    "        for i in range(num_samples):\n",
    "            V[str(i)] = {}\n",
    "            for j in range(self.setup.K): \n",
    "                B_inv = plus(B[str(i)]) @ self.X_V[str(j)].to(torch.cdouble) + B[str(i)] @ self.Y_V[str(j)].to(torch.cdouble) + self.Z_V[str(j)].to(torch.cdouble)\n",
    "                V[str(i)][str(j)] = B_inv @ H.iloc[i, j].conj().T @ U[str(i)][str(j)] @ W[str(i)][str(j)] + self.O_V[str(j)].to(torch.cdouble)\n",
    "\n",
    "        # Project V\n",
    "        V_proj = proj_power(V)\n",
    "        # V_proj = proj_power_soft(V, self.setup.PT, alpha=0.1)\n",
    "\n",
    "        return V_proj\n",
    "\n",
    "# Define the deep unfolding NN\n",
    "class DUNN1(nn.Module):\n",
    "    def __init__(self, num_layers, setup):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            Layer1(setup, i)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, V, H):\n",
    "        for layer in self.layers:\n",
    "            V = layer(V, H)\n",
    "        return V\n",
    "\n",
    "# Train the model\n",
    "class Trainer1:\n",
    "    def __init__(self, model: DUNN1, setup, lr: float = 1e-3):\n",
    "        self.setup = setup\n",
    "        self.model = model\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def train_epoch(self, dset, loss_fn, num_epochs, batch_size):\n",
    "\n",
    "        # Function for initializing V\n",
    "        # def init_V(H_df, setup):\n",
    "        #     num_samples = H_df.shape[0]\n",
    "        #     K = setup.K\n",
    "        #     V_dict = {}\n",
    "\n",
    "        #     for sample_idx in range(num_samples):\n",
    "        #         V_sample = {}\n",
    "        #         H_sample = [H_df.iloc[sample_idx, k] for k in range(K)]\n",
    "\n",
    "        #         for k in range(K):\n",
    "        #             # Create interference channel matrix for all users ≠ k\n",
    "        #             H_interference = torch.cat([H_sample[j] for j in range(K) if j != k], dim=0)\n",
    "\n",
    "        #             # Compute null space of interference channel using SVD\n",
    "        #             _, S, Vh = torch.linalg.svd(H_interference)\n",
    "        #             rank = (S > 1e-6).sum().item()\n",
    "        #             null_space = Vh[rank:].conj().T  # shape: [n_tx, nullity]\n",
    "\n",
    "        #             # Choose as many columns as the number of streams we want (≤ nullity)\n",
    "        #             d_k = min(setup.n_rx[k], null_space.shape[1])\n",
    "        #             V_k = null_space[:, :d_k]\n",
    "\n",
    "        #             V_sample[str(k)] = V_k\n",
    "\n",
    "        #         V_dict[str(sample_idx)] = V_sample\n",
    "        #     return V_dict\n",
    "\n",
    "        def proj_power(V):\n",
    "            for i in range(len(V)):\n",
    "                s = 0\n",
    "                for j in range(self.setup.K):\n",
    "                    s += torch.trace(V[str(i)][str(j)] @ V[str(i)][str(j)].conj().T)\n",
    "                for j in range(self.setup.K):\n",
    "                    V[str(i)][str(j)] = torch.sqrt(self.setup.PT/s) * V[str(i)][str(j)]\n",
    "            return V\n",
    "\n",
    "        def shuffle_and_batch(df, batch_size):\n",
    "            df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "            return [df_shuffled[i:i + batch_size] for i in range(0, len(df_shuffled), batch_size)]\n",
    "\n",
    "        loader = shuffle_and_batch(dset, batch_size)\n",
    "\n",
    "        H_total = dset.iloc[:, :self.setup.K]\n",
    "        V_total_df = dset.iloc[:, self.setup.K:(2*self.setup.K)]\n",
    "        V_total = {\n",
    "                    str(i): {str(j): V_total_df.iloc[i, j] for j in range(V_total_df.shape[1])}\n",
    "                    for i in range(len(V_total_df))\n",
    "                }\n",
    "        r_l = []\n",
    "        for _ in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for dset_batch in loader:\n",
    "                H_batch = dset_batch.iloc[:, :self.setup.K]\n",
    "                V_init_batch = dset_batch.iloc[:, self.setup.K:(2*self.setup.K)]\n",
    "                V0 = {\n",
    "                    str(i): {str(j): V_init_batch.iloc[i, j] for j in range(V_init_batch.shape[1])}\n",
    "                    for i in range(len(V_init_batch))\n",
    "                }\n",
    "                # H_batch = H_batch.to(torch.complex64)\n",
    "                self.opt.zero_grad()\n",
    "                # Initialize V\n",
    "                # V0 = init_V(H_batch, self.setup)\n",
    "                # V0 = proj_power(V0)\n",
    "                # V0 = V_init\n",
    "                V_pred = self.model(V0, H_batch)\n",
    "                loss = (-1) * loss_fn(H_batch, V_pred, self.setup.PT)\n",
    "                print(loss)\n",
    "                loss.backward()\n",
    "                # print(self.model.layers[0].X_U['0'].grad)\n",
    "                self.opt.step()\n",
    "                total_loss += loss.item()\n",
    "            # r_l.append(loss_fn(H_total, self.model(V_total, H_total), self.setup.PT))\n",
    "\n",
    "        # return total_loss / len(loader)\n",
    "        # return r_l\n",
    "        return V_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "502bac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the deep-unfolding NN architecture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the layer\n",
    "class Layer2(nn.Module):\n",
    "    def __init__(self, setup, flagg):\n",
    "        super().__init__()\n",
    "        self.setup = setup\n",
    "        X_U_dict = {}\n",
    "        Y_U_dict = {}\n",
    "        Z_U_dict = {}\n",
    "        O_U_dict = {}\n",
    "        X_W_dict = {}\n",
    "        Y_W_dict = {}\n",
    "        Z_W_dict = {}\n",
    "        X_V_dict = {}\n",
    "        Y_V_dict = {}\n",
    "        Z_V_dict = {}\n",
    "        O_V_dict = {}\n",
    "\n",
    "        if flagg == 0:\n",
    "            # print('h')\n",
    "            for i in range(self.setup.K):\n",
    "                X_U_dict[str(i)] = nn.Parameter(m1)\n",
    "                Y_U_dict[str(i)] = nn.Parameter(m2)\n",
    "                Z_U_dict[str(i)] = nn.Parameter(m3)\n",
    "                O_U_dict[str(i)] = nn.Parameter(m4)\n",
    "                X_W_dict[str(i)] = nn.Parameter(m5)\n",
    "                Y_W_dict[str(i)] = nn.Parameter(m6)\n",
    "                Z_W_dict[str(i)] = nn.Parameter(m7)\n",
    "                X_V_dict[str(i)] = nn.Parameter(m8)\n",
    "                Y_V_dict[str(i)] = nn.Parameter(m9)\n",
    "                Z_V_dict[str(i)] = nn.Parameter(m10)\n",
    "                O_V_dict[str(i)] = nn.Parameter(m11)\n",
    "                # X_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # Y_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # Z_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # O_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # X_W_dict[str(i)] = nn.Parameter(torch.randn(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # Y_W_dict[str(i)] = nn.Parameter(torch.randn(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # Z_W_dict[str(i)] = nn.Parameter(torch.randn(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # X_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # Y_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # Z_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # O_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.d[i], dtype=torch.cdouble))\n",
    "                # # X_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # # Y_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # # Z_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # # O_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # # X_W_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # # Y_W_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # # Z_W_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # # X_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # # Y_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # # Z_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # # O_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.d[i], dtype=torch.cdouble))\n",
    "            self.X_U = nn.ParameterDict(X_U_dict)\n",
    "            self.Y_U = nn.ParameterDict(Y_U_dict)\n",
    "            self.Z_U = nn.ParameterDict(Z_U_dict)\n",
    "            self.O_U = nn.ParameterDict(O_U_dict)\n",
    "            self.X_W = nn.ParameterDict(X_W_dict)\n",
    "            self.Y_W = nn.ParameterDict(Y_W_dict)\n",
    "            self.Z_W = nn.ParameterDict(Z_W_dict)\n",
    "            self.X_V = nn.ParameterDict(X_V_dict)\n",
    "            self.Y_V = nn.ParameterDict(Y_V_dict)\n",
    "            self.Z_V = nn.ParameterDict(Z_V_dict)\n",
    "            self.O_V = nn.ParameterDict(O_V_dict)\n",
    "\n",
    "        if flagg == 1:\n",
    "            # print('h')\n",
    "            for i in range(self.setup.K):\n",
    "                X_U_dict[str(i)] = nn.Parameter(m01)\n",
    "                Y_U_dict[str(i)] = nn.Parameter(m02)\n",
    "                Z_U_dict[str(i)] = nn.Parameter(m03)\n",
    "                O_U_dict[str(i)] = nn.Parameter(m04)\n",
    "                X_W_dict[str(i)] = nn.Parameter(m05)\n",
    "                Y_W_dict[str(i)] = nn.Parameter(m06)\n",
    "                Z_W_dict[str(i)] = nn.Parameter(m07)\n",
    "                X_V_dict[str(i)] = nn.Parameter(m08)\n",
    "                Y_V_dict[str(i)] = nn.Parameter(m09)\n",
    "                Z_V_dict[str(i)] = nn.Parameter(m010)\n",
    "                O_V_dict[str(i)] = nn.Parameter(m011)\n",
    "                # X_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # Y_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # Z_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # O_U_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_rx[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # X_W_dict[str(i)] = nn.Parameter(torch.randn(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # Y_W_dict[str(i)] = nn.Parameter(torch.randn(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # Z_W_dict[str(i)] = nn.Parameter(torch.randn(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # X_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # Y_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # Z_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # O_V_dict[str(i)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.d[i], dtype=torch.cdouble))\n",
    "                # # X_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # # Y_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # # Z_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.n_rx[i], dtype=torch.cdouble))\n",
    "                # # O_U_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_rx[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # # X_W_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # # Y_W_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # # Z_W_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.d[i], self.setup.d[i], dtype=torch.cdouble))\n",
    "                # # X_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # # Y_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # # Z_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.n_tx, dtype=torch.cdouble))\n",
    "                # # O_V_dict[str(i)] = nn.Parameter(100 * torch.rand(self.setup.n_tx, self.setup.d[i], dtype=torch.cdouble))\n",
    "            self.X_U = nn.ParameterDict(X_U_dict)\n",
    "            self.Y_U = nn.ParameterDict(Y_U_dict)\n",
    "            self.Z_U = nn.ParameterDict(Z_U_dict)\n",
    "            self.O_U = nn.ParameterDict(O_U_dict)\n",
    "            self.X_W = nn.ParameterDict(X_W_dict)\n",
    "            self.Y_W = nn.ParameterDict(Y_W_dict)\n",
    "            self.Z_W = nn.ParameterDict(Z_W_dict)\n",
    "            self.X_V = nn.ParameterDict(X_V_dict)\n",
    "            self.Y_V = nn.ParameterDict(Y_V_dict)\n",
    "            self.Z_V = nn.ParameterDict(Z_V_dict)\n",
    "            self.O_V = nn.ParameterDict(O_V_dict)\n",
    "\n",
    "\n",
    "    def forward(self, V, H):\n",
    "\n",
    "        # A^+ operator\n",
    "        # def plus(A):\n",
    "        #     diag_inv = 1.0 / A.diagonal(dim1=-2, dim2=-1)\n",
    "        #     A_plus = torch.zeros_like(A)\n",
    "        #     A_plus.diagonal(dim1=-2, dim2=-1).copy_(diag_inv)\n",
    "        #     return A_plus\n",
    "        # def plus(A: torch.Tensor) -> torch.Tensor:\n",
    "        #     \"\"\"\n",
    "        #     Moore-Penrose pseudo-inverse of a diagonal matrix: keeps autograd path.\n",
    "        #     Works batch-wise for ...×N×N tensors.\n",
    "        #     \"\"\"\n",
    "        #     diag = torch.diagonal(A, 0, -2, -1)          # keeps grad wrt A\n",
    "        #     inv  = 1.0 / diag                            # element-wise inverse\n",
    "        #     return torch.diag_embed(inv)\n",
    "        def plus(A):\n",
    "            return A\n",
    "\n",
    "        def proj_power(V):\n",
    "            V_proj = {}\n",
    "            for i in range(num_samples):\n",
    "                V_proj[str(i)] = {}\n",
    "                s = 0\n",
    "                for j in range(self.setup.K):\n",
    "                    s += torch.trace(V[str(i)][str(j)] @ V[str(i)][str(j)].conj().T)\n",
    "                for j in range(self.setup.K):\n",
    "                    V_proj[str(i)][str(j)] = torch.sqrt(self.setup.PT/s) * V[str(i)][str(j)]\n",
    "            return V_proj\n",
    "\n",
    "        # def proj_power(V_in: dict) -> dict:\n",
    "        #     \"\"\"\n",
    "        #     Return a *new* dictionary whose entries are power-normalised copies\n",
    "        #     of the Tx precoders in V_in.  No in-place writes → autograd path stays\n",
    "        #     intact when multiple unfolded layers are stacked.\n",
    "\n",
    "        #     V_in structure (unchanged):  {sample: {user: V_k}}\n",
    "        #     \"\"\"\n",
    "        #     V_out = {}\n",
    "        #     for sample, users in V_in.items():\n",
    "\n",
    "        #         # total power  (trace is complex → take real part)\n",
    "        #         P_tot = sum(torch.trace(v @ v.conj().T).real for v in users.values())\n",
    "        #         scale = torch.sqrt(self.setup.PT / (P_tot + 1e-12))     # +ε avoids div-by-0\n",
    "\n",
    "        #         V_out[sample] = {user: scale * v             # *clone* not needed\n",
    "        #                         for user, v in users.items()}\n",
    "        #     return V_out\n",
    "\n",
    "        # def proj_power_soft(V_in: dict, PT: float, alpha: float = 0.1) -> dict:\n",
    "        #     V_out = {}\n",
    "        #     for i, users in V_in.items():\n",
    "        #         P_tot = sum(torch.trace(v @ v.conj().T).real for v in users.values())\n",
    "        #         scale = torch.sqrt(PT / (P_tot + 1e-12))\n",
    "        #         V_out[str(i)] = {\n",
    "        #             str(k): (1 - alpha) * v + alpha * scale * v for k, v in users.items()\n",
    "        #         }\n",
    "        #     return V_out\n",
    "\n",
    "\n",
    "        num_samples = len(H)\n",
    "\n",
    "        # Calculate A\n",
    "        A = {}\n",
    "        for i in range(num_samples):\n",
    "            A[str(i)] = {}\n",
    "            for j in range(self.setup.K):\n",
    "                s = 0\n",
    "                for k in range(self.setup.K):\n",
    "                    s += torch.trace(V[str(i)][str(k)].conj().T @ V[str(i)][str(k)])\n",
    "                ey = (1/self.setup.PT) * s * torch.eye(self.setup.n_rx[j], dtype=torch.cdouble)\n",
    "                s = 0\n",
    "                for k in range(self.setup.K):\n",
    "                    s += V[str(i)][str(k)] @ V[str(i)][str(k)].conj().T\n",
    "                A[str(i)][str(j)] = ey + H.iloc[i, j] @ s @ H.iloc[i, j].conj().T\n",
    "\n",
    "        # Calculate U\n",
    "        U = {}\n",
    "        for i in range(num_samples):\n",
    "            U[str(i)] = {}\n",
    "            for j in range(self.setup.K):\n",
    "                A_inv = plus(A[str(i)][str(j)]) @ self.X_U[str(j)].to(torch.cdouble) + A[str(i)][str(j)] @ self.Y_U[str(j)].to(torch.cdouble) + self.Z_U[str(j)].to(torch.cdouble)\n",
    "                U[str(i)][str(j)] = A_inv @ H.iloc[i, j] @ V[str(i)][str(j)] + self.O_U[str(j)].to(torch.cdouble)\n",
    "\n",
    "        # Calclate E\n",
    "        E = {}\n",
    "        for i in range(num_samples):\n",
    "            E[str(i)] = {}\n",
    "            for j in range(self.setup.K):\n",
    "                E[str(i)][str(j)] = torch.eye(self.setup.d[j], dtype=torch.cdouble) - U[str(i)][str(j)].conj().T @ H.iloc[i, j] @ V[str(i)][str(j)]\n",
    "        \n",
    "        # Calculate W\n",
    "        W = {}\n",
    "        for i in range(num_samples):\n",
    "            W[str(i)] = {}\n",
    "            for j in range(self.setup.K):\n",
    "                W[str(i)][str(j)] = plus(E[str(i)][str(j)]) @ self.X_W[str(j)].to(torch.cdouble) + E[str(i)][str(j)] @ self.Y_W[str(j)].to(torch.cdouble) + self.Z_W[str(j)].to(torch.cdouble)\n",
    "\n",
    "        # Calculate B\n",
    "        B = {}\n",
    "        for i in range(num_samples):\n",
    "            s = 0\n",
    "            for k in range(self.setup.K):\n",
    "                s += torch.trace(U[str(i)][str(k)] @ W[str(i)][str(k)] @ U[str(i)][str(k)].conj().T)\n",
    "            ey = (1/self.setup.PT) * s * torch.eye(self.setup.n_tx, dtype=torch.cdouble)\n",
    "            s = 0\n",
    "            for k in range(self.setup.K):\n",
    "                s += H.iloc[i, k].conj().T @ U[str(i)][str(k)] @ W[str(i)][str(k)] @ U[str(i)][str(k)].conj().T @ H.iloc[i, k]\n",
    "            B[str(i)] = ey + s\n",
    "                \n",
    "\n",
    "        # Calculate V\n",
    "        V = {}\n",
    "        for i in range(num_samples):\n",
    "            V[str(i)] = {}\n",
    "            for j in range(self.setup.K): \n",
    "                B_inv = plus(B[str(i)]) @ self.X_V[str(j)].to(torch.cdouble) + B[str(i)] @ self.Y_V[str(j)].to(torch.cdouble) + self.Z_V[str(j)].to(torch.cdouble)\n",
    "                V[str(i)][str(j)] = B_inv @ H.iloc[i, j].conj().T @ U[str(i)][str(j)] @ W[str(i)][str(j)] + self.O_V[str(j)].to(torch.cdouble)\n",
    "\n",
    "        # Project V\n",
    "        V_proj = proj_power(V)\n",
    "        # V_proj = proj_power_soft(V, self.setup.PT, alpha=0.1)\n",
    "\n",
    "        return V_proj\n",
    "\n",
    "# Define the deep unfolding NN\n",
    "class DUNN2(nn.Module):\n",
    "    def __init__(self, num_layers, setup):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            Layer2(setup, i)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, V, H):\n",
    "        for layer in self.layers:\n",
    "            V = layer(V, H)\n",
    "        return V\n",
    "\n",
    "# Train the model\n",
    "class Trainer2:\n",
    "    def __init__(self, model: DUNN2, setup, lr: float = 1e-3):\n",
    "        self.setup = setup\n",
    "        self.model = model\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def train_epoch(self, dset, loss_fn, num_epochs, batch_size):\n",
    "\n",
    "        # Function for initializing V\n",
    "        # def init_V(H_df, setup):\n",
    "        #     num_samples = H_df.shape[0]\n",
    "        #     K = setup.K\n",
    "        #     V_dict = {}\n",
    "\n",
    "        #     for sample_idx in range(num_samples):\n",
    "        #         V_sample = {}\n",
    "        #         H_sample = [H_df.iloc[sample_idx, k] for k in range(K)]\n",
    "\n",
    "        #         for k in range(K):\n",
    "        #             # Create interference channel matrix for all users ≠ k\n",
    "        #             H_interference = torch.cat([H_sample[j] for j in range(K) if j != k], dim=0)\n",
    "\n",
    "        #             # Compute null space of interference channel using SVD\n",
    "        #             _, S, Vh = torch.linalg.svd(H_interference)\n",
    "        #             rank = (S > 1e-6).sum().item()\n",
    "        #             null_space = Vh[rank:].conj().T  # shape: [n_tx, nullity]\n",
    "\n",
    "        #             # Choose as many columns as the number of streams we want (≤ nullity)\n",
    "        #             d_k = min(setup.n_rx[k], null_space.shape[1])\n",
    "        #             V_k = null_space[:, :d_k]\n",
    "\n",
    "        #             V_sample[str(k)] = V_k\n",
    "\n",
    "        #         V_dict[str(sample_idx)] = V_sample\n",
    "        #     return V_dict\n",
    "\n",
    "        def proj_power(V):\n",
    "            for i in range(len(V)):\n",
    "                s = 0\n",
    "                for j in range(self.setup.K):\n",
    "                    s += torch.trace(V[str(i)][str(j)] @ V[str(i)][str(j)].conj().T)\n",
    "                for j in range(self.setup.K):\n",
    "                    V[str(i)][str(j)] = torch.sqrt(self.setup.PT/s) * V[str(i)][str(j)]\n",
    "            return V\n",
    "\n",
    "        def shuffle_and_batch(df, batch_size):\n",
    "            df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "            return [df_shuffled[i:i + batch_size] for i in range(0, len(df_shuffled), batch_size)]\n",
    "\n",
    "        loader = shuffle_and_batch(dset, batch_size)\n",
    "\n",
    "        H_total = dset.iloc[:, :self.setup.K]\n",
    "        V_total_df = dset.iloc[:, self.setup.K:(2*self.setup.K)]\n",
    "        V_total = {\n",
    "                    str(i): {str(j): V_total_df.iloc[i, j] for j in range(V_total_df.shape[1])}\n",
    "                    for i in range(len(V_total_df))\n",
    "                }\n",
    "        r_l = []\n",
    "        for _ in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for dset_batch in loader:\n",
    "                H_batch = dset_batch.iloc[:, :self.setup.K]\n",
    "                V_init_batch = dset_batch.iloc[:, self.setup.K:(2*self.setup.K)]\n",
    "                V0 = {\n",
    "                    str(i): {str(j): V_init_batch.iloc[i, j] for j in range(V_init_batch.shape[1])}\n",
    "                    for i in range(len(V_init_batch))\n",
    "                }\n",
    "                # H_batch = H_batch.to(torch.complex64)\n",
    "                self.opt.zero_grad()\n",
    "                # Initialize V\n",
    "                # V0 = init_V(H_batch, self.setup)\n",
    "                # V0 = proj_power(V0)\n",
    "                # V0 = V_init\n",
    "                V_pred = self.model(V0, H_batch)\n",
    "                loss = (-1) * loss_fn(H_batch, V_pred, self.setup.PT)\n",
    "                print(loss)\n",
    "                loss.backward()\n",
    "                # print(self.model.layers[0].X_U['0'].grad)\n",
    "                self.opt.step()\n",
    "                total_loss += loss.item()\n",
    "            # r_l.append(loss_fn(H_total, self.model(V_total, H_total), self.setup.PT))\n",
    "\n",
    "        # return total_loss / len(loader)\n",
    "        # return r_l\n",
    "        return V_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a10ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_power(V, PT_sc):\n",
    "    num_users_sc = len(V)\n",
    "    # Projects V according to the constraint\n",
    "    alph = torch.sqrt(torch.tensor(PT_sc)) / torch.sqrt(torch.tensor(sum([torch.trace(V[str(k)] @ V[str(k)].conj().T) for k in range(num_users_sc)])))\n",
    "    V_proj = {str(k): alph * V[str(k)] for k in range(num_users_sc)}\n",
    "    return V_proj\n",
    "\n",
    "def init_V(H):\n",
    "    # Initializes V according to Hu's code\n",
    "    V = {}\n",
    "    for k in range(len(H_dict)):\n",
    "        V[str(k)] = (torch.linalg.pinv(H[str(k)] @ H[str(k)].conj().T) @ H[str(k)]).conj().T\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0526923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_rate_loss_BC(H, V, PT):\n",
    "    # Recieves H as a dataframe of num_samples rows and num_users columns, V as a dict of num_samples keys, the value of each is a dict of num_users keys and their corresponding V as values. Outputs the average of sum rate across samples.\n",
    "    num_samples = len(H)\n",
    "    K = H.shape[1]\n",
    "    n_tx = H.iloc[0, 0].shape[1]\n",
    "    n_rx = [H.iloc[0, i].shape[0] for i in range(K)]\n",
    "    s_rate = []\n",
    "    for i in range(num_samples):\n",
    "        s = 0\n",
    "        for j in range(K):\n",
    "            ey = torch.eye(n_rx[j], dtype=torch.cfloat)\n",
    "            Omeg1 = torch.zeros((set_up.n_rx[0], set_up.n_rx[0]), dtype=torch.cdouble)\n",
    "            # Omeg1 = 0\n",
    "            # for k in range(K):\n",
    "            #     if k == j: pass\n",
    "            #     else:\n",
    "            #         Omeg1 += V[str(i)][str(k)] @ V[str(i)][str(k)].conj().T\n",
    "            # Omeg1 = H.iloc[i, j] @ Omeg1 @ H.iloc[i, j].conj().T\n",
    "            Omeg2 = 0\n",
    "            for k in range(K):\n",
    "                Omeg2 += torch.trace(V[str(i)][str(k)] @ V[str(i)][str(k)].conj().T)\n",
    "            Omeg2 = (1/PT) * Omeg2 * ey\n",
    "            Omeg = torch.linalg.inv(Omeg1 + Omeg2)\n",
    "            tmp = ey + H.iloc[i, j] @ V[str(i)][str(j)] @ V[str(i)][str(j)].conj().T @ H.iloc[i, j].conj().T @ Omeg\n",
    "            rate = (1/torch.log(torch.tensor(2.0))) * torch.logdet(tmp)\n",
    "            s += rate\n",
    "        s_rate.append(s)\n",
    "    return (sum(s_rate)/len(s_rate)).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f48bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class setup():\n",
    "    def __init__(self, n_tx, n_rx, num_streams, num_users, PT, sig):\n",
    "        self.n_tx = n_tx\n",
    "        self.n_rx = n_rx\n",
    "        self.d = num_streams\n",
    "        self.K = num_users\n",
    "        self.PT = PT\n",
    "        self.sig = sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc4945",
   "metadata": {},
   "source": [
    "Getting the first layer's optimal params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d6f9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The setup\n",
    "num_users = 1\n",
    "n_tx = 4\n",
    "n_rx = [2] * num_users\n",
    "d = [2] * num_users\n",
    "PT = 200\n",
    "sig = [1] * num_users\n",
    "alpha = [1] * num_users\n",
    "max_iter_alg = 100\n",
    "tol_alg = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a30e0b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for _ in range(1):  # 5 rows\n",
    "    row = {f'user_{i}': torch.randn(n_rx[i], n_tx, dtype=torch.cdouble) for i in range(num_users)}\n",
    "    data.append(row)\n",
    "\n",
    "H = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ad228e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/f3dbnk6d3ts1993_z878dt5w0000gn/T/ipykernel_89501/1777903577.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  H_dict = {str(i): row[i] for i in range(len(row))}\n",
      "/var/folders/35/f3dbnk6d3ts1993_z878dt5w0000gn/T/ipykernel_89501/4217901526.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alph = torch.sqrt(torch.tensor(PT_sc)) / torch.sqrt(torch.tensor(sum([torch.trace(V[str(k)] @ V[str(k)].conj().T) for k in range(num_users_sc)])))\n",
      "/Users/Ali/Projects/Deep-Unfolding-NN/src/sc_wmmse.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alph = torch.sqrt(torch.tensor(self.PT)) / torch.sqrt(torch.tensor(sum([torch.trace(V[str(k)] @ V[str(k)].conj().T) for k in range(self.K)])))\n"
     ]
    }
   ],
   "source": [
    "V_col = []\n",
    "V_init_col = []\n",
    "\n",
    "for idx, row in H.iterrows():\n",
    "    H_dict = {str(i): row[i] for i in range(len(row))}\n",
    "    wmm = WMMSE_alg_sc(K=num_users, n_tx=n_tx, n_rx=n_rx, H=H_dict, PT=PT, sig_k=sig, d=d, alpha=alpha, max_iter_alg=max_iter_alg, tol_alg=tol_alg)\n",
    "    V_init = init_V(H_dict)\n",
    "    V_init_proj = proj_power(V_init, PT)\n",
    "    V_l, U_l, W_l = wmm.algorithm(V_init_proj)\n",
    "    V_init_col.append(V_init_proj)\n",
    "    V_col.append(V_l[-1])\n",
    "\n",
    "V_df = pd.DataFrame(V_col)\n",
    "V_init_df = pd.DataFrame(V_init_col)\n",
    "\n",
    "dset = pd.concat([H, V_init_df, V_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fe0e640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/f3dbnk6d3ts1993_z878dt5w0000gn/T/ipykernel_89501/2648689252.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  H_dict = {str(i): row[i] for i in range(len(row))}\n",
      "/var/folders/35/f3dbnk6d3ts1993_z878dt5w0000gn/T/ipykernel_89501/4217901526.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alph = torch.sqrt(torch.tensor(PT_sc)) / torch.sqrt(torch.tensor(sum([torch.trace(V[str(k)] @ V[str(k)].conj().T) for k in range(num_users_sc)])))\n",
      "/Users/Ali/Projects/Deep-Unfolding-NN/src/sc_wmmse.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alph = torch.sqrt(torch.tensor(self.PT)) / torch.sqrt(torch.tensor(sum([torch.trace(V[str(k)] @ V[str(k)].conj().T) for k in range(self.K)])))\n"
     ]
    }
   ],
   "source": [
    "s_rate_l = []\n",
    "for idx, row in H.iterrows():\n",
    "    s_rate = 0\n",
    "    H_dict = {str(i): row[i] for i in range(len(row))}\n",
    "    wmm = WMMSE_alg_sc(K=num_users, n_tx=n_tx, n_rx=n_rx, H=H_dict, PT=PT, sig_k=sig, d=d, alpha=alpha, max_iter_alg=max_iter_alg, tol_alg=tol_alg)\n",
    "    V_init = init_V(H_dict)\n",
    "    V_init_proj = proj_power(V_init, PT)\n",
    "    V_l, U_l, W_l = wmm.algorithm(V_init_proj)\n",
    "    for i in range(num_users):\n",
    "        s_rate += torch.log2(torch.linalg.det(W_l[-1][str(i)]))\n",
    "    s_rate_l.append(s_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4f928385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(15.9009-1.3651e-14j, dtype=torch.complex128)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_rate_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c07d6437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.1687, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-14.7768, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-11.7760, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-13.6155, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.7858, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.1421, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-14.8624, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.3137, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.7466, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.7607, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.7465, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8513, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8787, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8125, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.7902, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8382, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8898, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9191, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9252, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8818, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8350, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8664, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9260, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9380, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9231, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9190, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9228, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9268, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9352, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9408, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9319, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9220, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9306, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9478, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9530, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9496, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9450, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9430, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9477, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9529, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9521, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9504, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9523, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9558, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9571, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9558, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9529, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9528, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9562, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9571, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9575, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9567, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9564, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9573, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9576, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9571, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9569, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9569, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9571, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9580, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9580, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9578, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9577, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9577, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9580, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9582, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9580, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9580, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9582, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9583, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9583, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9581, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9582, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9583, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9583, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9583, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9583, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9583, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9584, dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_up = setup(n_tx=n_tx, n_rx=n_rx, num_streams=d, num_users=num_users, PT=PT, sig = sig)\n",
    "\n",
    "du = DUNN1(num_layers=1, setup=set_up)\n",
    "\n",
    "tr = Trainer1(model=du, setup=set_up, lr=1e-1)\n",
    "V_final = tr.train_epoch(dset=dset, loss_fn=sum_rate_loss_BC, num_epochs=500, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6f3952c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = du.layers[0].X_U['0']\n",
    "m2 = du.layers[0].Y_U['0']\n",
    "m3 = du.layers[0].Z_U['0']\n",
    "m4 = du.layers[0].O_U['0']\n",
    "m5 = du.layers[0].X_W['0']\n",
    "m6 = du.layers[0].Y_W['0']\n",
    "m7 = du.layers[0].Z_W['0']\n",
    "m8 = du.layers[0].X_V['0']\n",
    "m9 = du.layers[0].Y_V['0']\n",
    "m10 = du.layers[0].Z_V['0']\n",
    "m11 = du.layers[0].O_V['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe1f060",
   "metadata": {},
   "source": [
    "Getting the seconds layer's optimal params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b96d50ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/f3dbnk6d3ts1993_z878dt5w0000gn/T/ipykernel_89501/4144476240.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  H_dict = {str(i): row[i] for i in range(len(row))}\n",
      "/var/folders/35/f3dbnk6d3ts1993_z878dt5w0000gn/T/ipykernel_89501/4217901526.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alph = torch.sqrt(torch.tensor(PT_sc)) / torch.sqrt(torch.tensor(sum([torch.trace(V[str(k)] @ V[str(k)].conj().T) for k in range(num_users_sc)])))\n"
     ]
    }
   ],
   "source": [
    "V_final[str(0)][str(0)] = V_final[str(0)][str(0)].detach()\n",
    "\n",
    "V_col = []\n",
    "V_init_col = []\n",
    "\n",
    "for idx, row in H.iterrows():\n",
    "    H_dict = {str(i): row[i] for i in range(len(row))}\n",
    "    wmm = WMMSE_alg_sc(K=num_users, n_tx=n_tx, n_rx=n_rx, H=H_dict, PT=PT, sig_k=sig, d=d, alpha=alpha, max_iter_alg=max_iter_alg, tol_alg=tol_alg)\n",
    "    V_init = init_V(H_dict)\n",
    "    V_init_proj = proj_power(V_init, PT)\n",
    "    V_l, U_l, W_l = wmm.algorithm(V_init_proj)\n",
    "    # V_init_col.append(V_init_proj)\n",
    "    V_init_col.append(V_final[str(0)])\n",
    "    V_col.append(V_l[-1])\n",
    "\n",
    "V_df = pd.DataFrame(V_col)\n",
    "V_init_df = pd.DataFrame(V_init_col)\n",
    "\n",
    "dset = pd.concat([H, V_init_df, V_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "746e2c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.0490, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.0572, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.0653, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.0733, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.0811, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.0888, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.0964, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.1038, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.1110, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.1180, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.1248, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.1316, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.1386, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.1462, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.1558, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.1724, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2224, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.5336, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-11.7472, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-14.9998, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-13.1447, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-14.6882, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-14.6439, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-14.0640, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-14.9623, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.3451, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-14.7486, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-14.9590, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.5599, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.3841, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.1117, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.3597, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.7068, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.5799, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.4222, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.5795, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.7785, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.7445, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.6450, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.6830, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8165, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8679, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8028, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.7774, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8514, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9171, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8955, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8576, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.8773, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9265, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9402, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9162, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9047, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9267, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9498, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9440, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9261, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9275, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9453, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9515, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9399, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9328, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9408, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9493, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9462, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9393, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9399, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9464, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9488, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9450, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9425, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9460, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9500, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9490, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9465, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9476, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9508, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9515, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9500, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9497, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9515, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9528, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9522, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9514, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9521, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9532, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9532, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9525, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9525, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9533, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9535, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9530, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9528, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9532, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9535, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9533, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9530, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9532, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9535, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9534, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9532, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9533, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9535, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9535, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9533, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9534, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9535, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9536, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9535, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9535, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9536, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9536, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9536, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9536, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9537, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9538, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9539, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9540, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9541, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9542, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9543, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9544, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9545, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9546, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9547, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9548, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9549, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9550, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9551, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9552, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9553, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-15.9554, dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_up = setup(n_tx=n_tx, n_rx=n_rx, num_streams=d, num_users=num_users, PT=PT, sig = sig)\n",
    "\n",
    "du = DUNN1(num_layers=1, setup=set_up)\n",
    "\n",
    "tr = Trainer1(model=du, setup=set_up, lr=1e-2)\n",
    "V_final = tr.train_epoch(dset=dset, loss_fn=sum_rate_loss_BC, num_epochs=500, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "18ac94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m01 = du.layers[0].X_U['0']\n",
    "m02 = du.layers[0].Y_U['0']\n",
    "m03 = du.layers[0].Z_U['0']\n",
    "m04 = du.layers[0].O_U['0']\n",
    "m05 = du.layers[0].X_W['0']\n",
    "m06 = du.layers[0].Y_W['0']\n",
    "m07 = du.layers[0].Z_W['0']\n",
    "m08 = du.layers[0].X_V['0']\n",
    "m09 = du.layers[0].Y_V['0']\n",
    "m010 = du.layers[0].Z_V['0']\n",
    "m011 = du.layers[0].O_V['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd745722",
   "metadata": {},
   "source": [
    "Running with two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a1aff301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/f3dbnk6d3ts1993_z878dt5w0000gn/T/ipykernel_89501/1777903577.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  H_dict = {str(i): row[i] for i in range(len(row))}\n",
      "/var/folders/35/f3dbnk6d3ts1993_z878dt5w0000gn/T/ipykernel_89501/4217901526.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alph = torch.sqrt(torch.tensor(PT_sc)) / torch.sqrt(torch.tensor(sum([torch.trace(V[str(k)] @ V[str(k)].conj().T) for k in range(num_users_sc)])))\n",
      "/Users/Ali/Projects/Deep-Unfolding-NN/src/sc_wmmse.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alph = torch.sqrt(torch.tensor(self.PT)) / torch.sqrt(torch.tensor(sum([torch.trace(V[str(k)] @ V[str(k)].conj().T) for k in range(self.K)])))\n"
     ]
    }
   ],
   "source": [
    "V_col = []\n",
    "V_init_col = []\n",
    "\n",
    "for idx, row in H.iterrows():\n",
    "    H_dict = {str(i): row[i] for i in range(len(row))}\n",
    "    wmm = WMMSE_alg_sc(K=num_users, n_tx=n_tx, n_rx=n_rx, H=H_dict, PT=PT, sig_k=sig, d=d, alpha=alpha, max_iter_alg=max_iter_alg, tol_alg=tol_alg)\n",
    "    V_init = init_V(H_dict)\n",
    "    V_init_proj = proj_power(V_init, PT)\n",
    "    V_l, U_l, W_l = wmm.algorithm(V_init_proj)\n",
    "    V_init_col.append(V_init_proj)\n",
    "    V_col.append(V_l[-1])\n",
    "\n",
    "V_df = pd.DataFrame(V_col)\n",
    "V_init_df = pd.DataFrame(V_init_col)\n",
    "\n",
    "dset = pd.concat([H, V_init_df, V_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "78581993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2666, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2669, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2668, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2670, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2670, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2670, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2670, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(-9.2671, dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_up = setup(n_tx=n_tx, n_rx=n_rx, num_streams=d, num_users=num_users, PT=PT, sig = sig)\n",
    "\n",
    "du = DUNN2(num_layers=2, setup=set_up)\n",
    "\n",
    "tr = Trainer2(model=du, setup=set_up, lr=1e-1)\n",
    "V_final = tr.train_epoch(dset=dset, loss_fn=sum_rate_loss_BC, num_epochs=500, batch_size=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-Unfolding-NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
