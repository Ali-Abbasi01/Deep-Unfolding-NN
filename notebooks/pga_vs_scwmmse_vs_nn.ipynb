{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc06253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the current working directory\n",
    "scripts_dir = os.getcwd()\n",
    "# Go up one level\n",
    "project_root = os.path.abspath(os.path.join(scripts_dir, '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import src.sc_wmmse\n",
    "importlib.reload(src.sc_wmmse)\n",
    "from src.sc_wmmse import WMMSE_alg_sc\n",
    "\n",
    "# import src.utils\n",
    "# importlib.reload(src.utils)\n",
    "# from src.utils import calculate_sum_rate_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c7c77",
   "metadata": {},
   "source": [
    "Data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class setup():\n",
    "    def __init__(self, n_tx, n_rx, num_streams, num_users, PT, sig, alpha):\n",
    "        self.n_tx = n_tx\n",
    "        self.n_rx = n_rx\n",
    "        self.d = num_streams\n",
    "        self.K = num_users\n",
    "        self.PT = PT\n",
    "        self.sig = sig\n",
    "        self.alpha = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e244cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The setup\n",
    "num_users = 1\n",
    "n_tx = 4\n",
    "n_rx = [2] * num_users\n",
    "d = [2] * num_users\n",
    "PT = 300\n",
    "sig = [1] * num_users\n",
    "alpha = [1] * num_users\n",
    "max_iter_alg = 1000\n",
    "tol_alg = 1e-4\n",
    "alpha = [1] * num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ebc2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_up = setup(n_tx, n_rx, d, num_users, PT, sig, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "26240de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = {str(i): torch.randn(set_up.n_rx[i], set_up.n_tx, dtype=torch.cdouble) for i in range(set_up.K)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "60a1a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_init = {str(i): torch.rand(set_up.n_tx, set_up.d[i], dtype=torch.cdouble) for i in range(set_up.K)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "696b954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_power(V):\n",
    "    s = torch.trace(V[str(0)] @ V[str(0)].conj().T)\n",
    "    V[str(0)] = torch.sqrt(set_up.PT/s) * V[str(0)]\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "0f5cc496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(300.+2.6073e-15j, dtype=torch.complex128)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_init_proj = proj_power(V_init)\n",
    "torch.trace(V_init_proj[str(0)] @ V_init_proj[str(0)].conj().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5913a",
   "metadata": {},
   "source": [
    "sc wmmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0f97fa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/Projects/Deep-Unfolding-NN/src/sc_wmmse.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alph = torch.sqrt(torch.tensor(self.PT)) / torch.sqrt(torch.tensor(sum([torch.trace(V[str(k)] @ V[str(k)].conj().T) for k in range(self.K)])))\n"
     ]
    }
   ],
   "source": [
    "w = WMMSE_alg_sc(set_up.K, set_up.n_tx, set_up.n_rx, H, set_up.PT, set_up.sig, set_up.d, set_up.alpha, max_iter_alg, tol_alg)\n",
    "V_l, U_l, W_l = w.algorithm(V_init_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "eb21fbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.4350-3.1882e-14j, dtype=torch.complex128)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log2(torch.linalg.det(W_l[-1][str(0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0e8633",
   "metadata": {},
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c2313c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sum_rate_sc(H, V, alpha, sig):\n",
    "    # Calculate sum rate for single cell\n",
    "    sum_rate = 0\n",
    "    for k in range(len(H)):\n",
    "        Nr = H[str(k)].shape[0]\n",
    "        # Calculate Omega\n",
    "        S = 0\n",
    "        for l in range(len(H)):\n",
    "            if l == k: pass\n",
    "            else:\n",
    "                S += H[str(k)] @ V[str(l)] @ V[str(l)].conj().T @ H[str(k)].conj().T\n",
    "        S += sig[k] * torch.eye(Nr, dtype=torch.cdouble)\n",
    "        tmp = torch.eye(Nr, dtype=torch.cdouble) + H[str(k)] @ V[str(k)] @ V[str(k)].conj().T @ H[str(k)].conj().T @ torch.linalg.inv(S)\n",
    "        R = torch.log2(torch.linalg.det(tmp))\n",
    "        sum_rate += alpha[k] * R\n",
    "    return sum_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a2fc8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, setup):\n",
    "        super().__init__()\n",
    "        self.setup = setup\n",
    "        dict = {}\n",
    "        dict[str(0)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.d[0], dtype=torch.cdouble))\n",
    "        self.V = nn.ParameterDict(dict)\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        return self.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "182787a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN with adam optimizer\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: Layer, setup, lr: float = 1e-3):\n",
    "        self.setup = setup\n",
    "        self.model = model\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def train_epoch(self, H, num_epochs):\n",
    "\n",
    "        def proj_power(V):\n",
    "            s = torch.trace(V[str(0)] @ V[str(0)].conj().T)\n",
    "            scale = torch.sqrt(self.setup.PT / s)\n",
    "            V[str(0)].mul_(scale)\n",
    "\n",
    "        V_l = []\n",
    "        for _ in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            self.opt.zero_grad()\n",
    "            loss = -1 * calculate_sum_rate_sc(H, self.model.V, self.setup.alpha, self.setup.sig).real\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.opt.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                proj_power(self.model.V)\n",
    "\n",
    "            V_l.append(self.model.V[str(0)].detach().clone())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return calculate_sum_rate_sc(H, self.model.V, self.setup.alpha, self.setup.sig), self.model(), V_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2ca5b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN with manual update\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: Layer, setup, lr: float = 1e-3):\n",
    "        self.setup = setup\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def train_epoch(self, H, num_epochs):\n",
    "\n",
    "        def proj_power(V):\n",
    "            s = torch.trace(V[str(0)] @ V[str(0)].conj().T)\n",
    "            scale = torch.sqrt(self.setup.PT / s)\n",
    "            V[str(0)].mul_(scale)\n",
    "\n",
    "        I = torch.eye(2, dtype=torch.cdouble)\n",
    "        V_l = []\n",
    "        for _ in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            self.model.V[str(0)].requires_grad_(True)\n",
    "\n",
    "            if self.model.V[str(0)].grad is not None:\n",
    "                self.model.V[str(0)].grad.zero_()\n",
    "\n",
    "            loss = -1 * calculate_sum_rate_sc(H, self.model.V, self.setup.alpha, self.setup.sig).real\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.model.V[str(0)] -= self.lr * self.model.V[str(0)].grad\n",
    "                proj_power(self.model.V)\n",
    "\n",
    "            V_l.append(self.model.V[str(0)].detach().clone())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return calculate_sum_rate_sc(H, self.model.V, self.setup.alpha, self.setup.sig), self.model(), V_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "50b2de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Trainer(model=Layer(set_up), setup=set_up, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "82353fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a2, a3 = tr.train_epoch(H, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f902e2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28bea48e0>]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_l = []\n",
    "for i in a3:\n",
    "    r_l.append(torch.log2(torch.linalg.det(torch.eye(2, dtype=torch.cdouble) + H['0'] @ i @ i.conj().T @ H['0'].conj().T)).real)\n",
    "plt.plot(r_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5cc17c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.4391, dtype=torch.float64)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(r_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "03492d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(300.0000-2.0878e-15j, dtype=torch.complex128)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(V_l[-1]['0'] @ V_l[-1]['0'].conj().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91fab6",
   "metadata": {},
   "source": [
    "2 layer nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "c3ff5ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.randn(3, 3, dtype=torch.cdouble))\n",
    "\n",
    "    def forward(self, V):\n",
    "\n",
    "        # def proj_power(V):\n",
    "        #     s = torch.trace(V[str(0)] @ V[str(0)].conj().T)\n",
    "        #     V[str(0)] = torch.sqrt(self.setup.PT/s) * V[str(0)]\n",
    "        #     return V\n",
    "\n",
    "        # return proj_power(self.V)\n",
    "        return self.A @ V\n",
    "\n",
    "class DUNN(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            Layer()\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, V):\n",
    "        for layer in self.layers:\n",
    "            V = layer(V)\n",
    "        return V\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: Layer, lr: float = 1e-3):\n",
    "        self.model = model\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # self.opt = torch.optim.LBFGS(self.model.parameters(), lr=.1, max_iter=20, line_search_fn='strong_wolfe')\n",
    "\n",
    "\n",
    "    def train_epoch(self, V, num_epochs):\n",
    "\n",
    "        V_l = []\n",
    "        for _ in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            self.opt.zero_grad()\n",
    "            loss = torch.trace(self.model(V).conj().T @  self.model(V)).real\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            # print(self.model.V[str(0)].grad)\n",
    "            # print(self.model.V[str(0)])\n",
    "            self.opt.step()\n",
    "            # print(self.model.V[str(0)])\n",
    "            # self.model.V = proj_power(self.model.V)\n",
    "            # print(self.model.V[str(0)], '\\n')\n",
    "            # print(self.model.V[str(0)] @ self.model.V[str(0)].conj().T, '\\n')\n",
    "            # with torch.no_grad():\n",
    "            #     proj_power(self.model.V)\n",
    "            # print(torch.log2(torch.linalg.det(torch.eye(2, dtype=torch.cdouble) + H['0'] @ self.model.V[str(0)] @ self.model.V[str(0)].conj().T @ H['0'].conj().T)), '\\n')\n",
    "            # print(self.model.V[str(0)], '\\n')\n",
    "            # V_l.append(self.model.V)\n",
    "            # V_l.append(self.model.V[str(0)].detach().clone())\n",
    "            # total_loss += loss.item()\n",
    "\n",
    "        # return calculate_sum_rate_sc(H, self.model.V, self.setup.alpha, self.setup.sig), self.model(), V_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "8194f723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.6534, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(17.8405, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(16.1942, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(14.7063, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(13.3632, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(12.1490, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(11.0510, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(10.0602, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(9.1690, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(8.3702, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(7.6562, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(7.0194, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(6.4519, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(5.9457, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(5.4929, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(5.0859, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(4.7177, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(4.3818, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(4.0725, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(3.7854, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(3.5169, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(3.2644, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(3.0265, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(2.8024, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(2.5917, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(2.3947, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(2.2116, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(2.0425, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.8875, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.7463, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.6186, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.5034, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.3998, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.3066, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.2226, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.1465, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.0771, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.0134, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.9546, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.8999, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.8489, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.8011, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.7564, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.7146, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.6756, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.6392, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.6055, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.5743, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.5456, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.5192, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4949, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4726, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4521, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4333, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4159, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3997, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3848, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3708, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3577, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3455, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3339, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3231, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3128, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3032, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2940, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2855, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2774, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2698, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2626, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2558, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2495, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2435, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2379, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2325, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2274, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2225, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2179, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2134, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2091, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2050, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2010, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1972, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1936, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1901, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1868, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1836, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1805, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1776, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1747, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1720, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1694, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1669, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1644, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1621, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1598, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1576, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1554, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1533, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1513, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1493, dtype=torch.float64, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "V = torch.randn(3, 3, dtype=torch.cdouble)\n",
    "\n",
    "du = DUNN(num_layers=2)\n",
    "\n",
    "tr = Trainer(model=du, lr=.01)\n",
    "tr.train_epoch(V, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce7c8f",
   "metadata": {},
   "source": [
    "pga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fcd0a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fixed_channel_pga():\n",
    "\n",
    "    def __init__(self, H, PT):\n",
    "        self.H = H\n",
    "        self.PT = PT\n",
    "        self.Nr, self.Nt = H.shape\n",
    "        \n",
    "    def solve(self, num_iter=200000, lr=0.1):\n",
    "        def proj_psd_trace(S, P):\n",
    "            \"\"\"Project Hermitian S onto {X ≽ 0,  tr(X) ≤ P}.\"\"\"\n",
    "            # Hermitian eigendecomp\n",
    "            eigval, eigvec = torch.linalg.eigh(S)\n",
    "            eigval.clamp_(min=0)             # PSD\n",
    "            s = eigval.sum()\n",
    "            if s > P:                        # scale down uniformly\n",
    "                eigval *= P / s\n",
    "            return (eigvec * eigval) @ eigvec.conj().T\n",
    "        \n",
    "        I = torch.eye(self.Nr, dtype=torch.cdouble)\n",
    "\n",
    "        Sigma = torch.eye(self.Nt, dtype=torch.cdouble, requires_grad=True)\n",
    "\n",
    "        for _ in range(num_iter): \n",
    "            Sigma.requires_grad_(True)\n",
    "            M = I + self.H @ Sigma @ self.H.conj().T\n",
    "            loss = torch.logdet(M).real\n",
    "            loss.backward()\n",
    "            g = Sigma.grad\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Sigma = Sigma + lr * g\n",
    "                Sigma = proj_psd_trace(Sigma, self.PT)\n",
    "\n",
    "        return Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "623740b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pga = fixed_channel_pga(H['0'], 300)\n",
    "sigma = pga.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "13873f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.4261-1.7270e-16j, dtype=torch.complex128)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log2(torch.linalg.det(torch.eye(2, dtype=torch.cdouble) + H['0'] @ sigma @ H['0'].conj().T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-Unfolding-NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
