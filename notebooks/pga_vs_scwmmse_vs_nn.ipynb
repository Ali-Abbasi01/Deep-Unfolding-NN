{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc06253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the current working directory\n",
    "scripts_dir = os.getcwd()\n",
    "# Go up one level\n",
    "project_root = os.path.abspath(os.path.join(scripts_dir, '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import src.sc_wmmse\n",
    "importlib.reload(src.sc_wmmse)\n",
    "from src.sc_wmmse import WMMSE_alg_sc\n",
    "\n",
    "# import src.utils\n",
    "# importlib.reload(src.utils)\n",
    "# from src.utils import calculate_sum_rate_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ca207",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "108c7c77",
   "metadata": {},
   "source": [
    "Data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class setup():\n",
    "    def __init__(self, n_tx, n_rx, num_streams, num_users, PT, sig, alpha):\n",
    "        self.n_tx = n_tx\n",
    "        self.n_rx = n_rx\n",
    "        self.d = num_streams\n",
    "        self.K = num_users\n",
    "        self.PT = PT\n",
    "        self.sig = sig\n",
    "        self.alpha = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e244cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The setup\n",
    "num_users = 1\n",
    "n_tx = 4\n",
    "n_rx = [2] * num_users\n",
    "d = [2] * num_users\n",
    "PT = 300\n",
    "sig = [1] * num_users\n",
    "alpha = [1] * num_users\n",
    "max_iter_alg = 1000\n",
    "tol_alg = 1e-4\n",
    "alpha = [1] * num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ebc2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_up = setup(n_tx, n_rx, d, num_users, PT, sig, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "26240de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = {str(i): torch.randn(set_up.n_rx[i], set_up.n_tx, dtype=torch.cdouble) for i in range(set_up.K)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "60a1a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_init = {str(i): torch.rand(set_up.n_tx, set_up.d[i], dtype=torch.cdouble) for i in range(set_up.K)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "696b954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_power(V):\n",
    "    s = torch.trace(V[str(0)] @ V[str(0)].conj().T)\n",
    "    V[str(0)] = torch.sqrt(set_up.PT/s) * V[str(0)]\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0f5cc496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(300.0000-4.3506e-16j, dtype=torch.complex128)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_init_proj = proj_power(V_init)\n",
    "torch.trace(V_init_proj[str(0)] @ V_init_proj[str(0)].conj().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5913a",
   "metadata": {},
   "source": [
    "sc wmmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0f97fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = WMMSE_alg_sc(set_up.K, set_up.n_tx, set_up.n_rx, H, set_up.PT, set_up.sig, set_up.d, set_up.alpha, max_iter_alg, tol_alg)\n",
    "V_l, U_l, W_l = w.algorithm(V_init_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "eb21fbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.6705-9.2446e-14j, dtype=torch.complex128)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log2(torch.linalg.det(W_l[-1][str(0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0e8633",
   "metadata": {},
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c2313c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sum_rate_sc(H, V, alpha, sig):\n",
    "    # Calculate sum rate for single cell\n",
    "    sum_rate = 0\n",
    "    for k in range(len(H)):\n",
    "        Nr = H[str(k)].shape[0]\n",
    "        # Calculate Omega\n",
    "        S = 0\n",
    "        for l in range(len(H)):\n",
    "            if l == k: pass\n",
    "            else:\n",
    "                S += H[str(k)] @ V[str(l)] @ V[str(l)].conj().T @ H[str(k)].conj().T\n",
    "        S += sig[k] * torch.eye(Nr, dtype=torch.cdouble)\n",
    "        tmp = torch.eye(Nr, dtype=torch.cdouble) + H[str(k)] @ V[str(k)] @ V[str(k)].conj().T @ H[str(k)].conj().T @ torch.linalg.inv(S)\n",
    "        R = torch.log2(torch.linalg.det(tmp))\n",
    "        sum_rate += alpha[k] * R\n",
    "    return sum_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a2fc8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, setup):\n",
    "        super().__init__()\n",
    "        self.setup = setup\n",
    "        dict = {}\n",
    "        dict[str(0)] = nn.Parameter(torch.randn(self.setup.n_tx, self.setup.d[0], dtype=torch.cdouble))\n",
    "        self.V = nn.ParameterDict(dict)\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        # def proj_power(V):\n",
    "        #     s = torch.trace(V[str(0)] @ V[str(0)].conj().T)\n",
    "        #     V[str(0)] = torch.sqrt(self.setup.PT/s) * V[str(0)]\n",
    "        #     return V\n",
    "\n",
    "        # return proj_power(self.V)\n",
    "        return self.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "182787a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: Layer, setup, lr: float = 1e-3):\n",
    "        self.setup = setup\n",
    "        self.model = model\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # self.opt = torch.optim.LBFGS(self.model.parameters(), lr=.1, max_iter=20, line_search_fn='strong_wolfe')\n",
    "\n",
    "\n",
    "    def train_epoch(self, H, num_epochs):\n",
    "\n",
    "        def proj_power(V):\n",
    "            s = torch.trace(V[str(0)] @ V[str(0)].conj().T)\n",
    "            scale = torch.sqrt(self.setup.PT / s)\n",
    "            V[str(0)].mul_(scale)\n",
    "\n",
    "        V_l = []\n",
    "        for _ in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            self.opt.zero_grad()\n",
    "            loss = -1 * calculate_sum_rate_sc(H, self.model.V, self.setup.alpha, self.setup.sig).real\n",
    "            loss.backward()\n",
    "            # print(self.model.V[str(0)].grad)\n",
    "            # print(self.model.V[str(0)])\n",
    "            self.opt.step()\n",
    "            # print(self.model.V[str(0)])\n",
    "            # self.model.V = proj_power(self.model.V)\n",
    "            # print(self.model.V[str(0)], '\\n')\n",
    "            # print(self.model.V[str(0)] @ self.model.V[str(0)].conj().T, '\\n')\n",
    "            with torch.no_grad():\n",
    "                proj_power(self.model.V)\n",
    "            # print(torch.log2(torch.linalg.det(torch.eye(2, dtype=torch.cdouble) + H['0'] @ self.model.V[str(0)] @ self.model.V[str(0)].conj().T @ H['0'].conj().T)), '\\n')\n",
    "            # print(self.model.V[str(0)], '\\n')\n",
    "            # V_l.append(self.model.V)\n",
    "            V_l.append(self.model.V[str(0)].detach().clone())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return calculate_sum_rate_sc(H, self.model.V, self.setup.alpha, self.setup.sig), self.model(), V_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Trainer(model=Layer(set_up), setup=set_up, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82353fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a2, a3 = tr.train_epoch(H, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l = []\n",
    "for i in a3:\n",
    "    r_l.append(torch.log2(torch.linalg.det(torch.eye(2, dtype=torch.cdouble) + H['0'] @ i @ i.conj().T @ H['0'].conj().T)).real)\n",
    "plt.plot(r_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc17c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(r_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03492d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trace(V_l[-1]['0'] @ V_l[-1]['0'].conj().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91fab6",
   "metadata": {},
   "source": [
    "2 layer nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "c3ff5ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.randn(3, 3, dtype=torch.cdouble))\n",
    "\n",
    "    def forward(self, V):\n",
    "\n",
    "        # def proj_power(V):\n",
    "        #     s = torch.trace(V[str(0)] @ V[str(0)].conj().T)\n",
    "        #     V[str(0)] = torch.sqrt(self.setup.PT/s) * V[str(0)]\n",
    "        #     return V\n",
    "\n",
    "        # return proj_power(self.V)\n",
    "        return self.A @ V\n",
    "\n",
    "class DUNN(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            Layer()\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, V):\n",
    "        for layer in self.layers:\n",
    "            V = layer(V)\n",
    "        return V\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: Layer, lr: float = 1e-3):\n",
    "        self.model = model\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # self.opt = torch.optim.LBFGS(self.model.parameters(), lr=.1, max_iter=20, line_search_fn='strong_wolfe')\n",
    "\n",
    "\n",
    "    def train_epoch(self, V, num_epochs):\n",
    "\n",
    "        V_l = []\n",
    "        for _ in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            self.opt.zero_grad()\n",
    "            loss = torch.trace(self.model(V).conj().T @  self.model(V)).real\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            # print(self.model.V[str(0)].grad)\n",
    "            # print(self.model.V[str(0)])\n",
    "            self.opt.step()\n",
    "            # print(self.model.V[str(0)])\n",
    "            # self.model.V = proj_power(self.model.V)\n",
    "            # print(self.model.V[str(0)], '\\n')\n",
    "            # print(self.model.V[str(0)] @ self.model.V[str(0)].conj().T, '\\n')\n",
    "            # with torch.no_grad():\n",
    "            #     proj_power(self.model.V)\n",
    "            # print(torch.log2(torch.linalg.det(torch.eye(2, dtype=torch.cdouble) + H['0'] @ self.model.V[str(0)] @ self.model.V[str(0)].conj().T @ H['0'].conj().T)), '\\n')\n",
    "            # print(self.model.V[str(0)], '\\n')\n",
    "            # V_l.append(self.model.V)\n",
    "            # V_l.append(self.model.V[str(0)].detach().clone())\n",
    "            # total_loss += loss.item()\n",
    "\n",
    "        # return calculate_sum_rate_sc(H, self.model.V, self.setup.alpha, self.setup.sig), self.model(), V_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "8194f723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.6534, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(17.8405, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(16.1942, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(14.7063, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(13.3632, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(12.1490, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(11.0510, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(10.0602, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(9.1690, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(8.3702, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(7.6562, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(7.0194, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(6.4519, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(5.9457, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(5.4929, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(5.0859, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(4.7177, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(4.3818, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(4.0725, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(3.7854, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(3.5169, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(3.2644, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(3.0265, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(2.8024, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(2.5917, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(2.3947, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(2.2116, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(2.0425, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.8875, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.7463, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.6186, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.5034, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.3998, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.3066, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.2226, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.1465, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.0771, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(1.0134, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.9546, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.8999, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.8489, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.8011, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.7564, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.7146, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.6756, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.6392, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.6055, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.5743, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.5456, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.5192, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4949, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4726, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4521, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4333, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4159, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3997, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3848, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3708, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3577, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3455, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3339, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3231, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3128, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.3032, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2940, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2855, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2774, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2698, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2626, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2558, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2495, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2435, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2379, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2325, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2274, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2225, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2179, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2134, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2091, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2050, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2010, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1972, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1936, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1901, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1868, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1836, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1805, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1776, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1747, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1720, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1694, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1669, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1644, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1621, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1598, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1576, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1554, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1533, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1513, dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "tensor(0.1493, dtype=torch.float64, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "V = torch.randn(3, 3, dtype=torch.cdouble)\n",
    "\n",
    "du = DUNN(num_layers=2)\n",
    "\n",
    "tr = Trainer(model=du, lr=.01)\n",
    "tr.train_epoch(V, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce7c8f",
   "metadata": {},
   "source": [
    "pga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fcd0a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fixed_channel_pga():\n",
    "\n",
    "    def __init__(self, H, PT):\n",
    "        self.H = H\n",
    "        self.PT = PT\n",
    "        self.Nr, self.Nt = H.shape\n",
    "        \n",
    "    def solve(self, num_iter=200000, lr=0.1):\n",
    "        def proj_psd_trace(S, P):\n",
    "            \"\"\"Project Hermitian S onto {X ≽ 0,  tr(X) ≤ P}.\"\"\"\n",
    "            # Hermitian eigendecomp\n",
    "            eigval, eigvec = torch.linalg.eigh(S)\n",
    "            eigval.clamp_(min=0)             # PSD\n",
    "            s = eigval.sum()\n",
    "            if s > P:                        # scale down uniformly\n",
    "                eigval *= P / s\n",
    "            return (eigvec * eigval) @ eigvec.conj().T\n",
    "        \n",
    "        I = torch.eye(self.Nr, dtype=torch.cdouble)\n",
    "\n",
    "        Sigma = torch.eye(self.Nt, dtype=torch.cdouble, requires_grad=True)\n",
    "\n",
    "        for _ in range(num_iter): \n",
    "            Sigma.requires_grad_(True)\n",
    "            M = I + self.H @ Sigma @ self.H.conj().T\n",
    "            loss = torch.logdet(M).real\n",
    "            loss.backward()\n",
    "            g = Sigma.grad\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Sigma = Sigma + lr * g\n",
    "                Sigma = proj_psd_trace(Sigma, self.PT)\n",
    "\n",
    "        return Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "623740b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pga = fixed_channel_pga(H['0'], 300)\n",
    "sigma = pga.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "13873f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.6782-4.3193e-18j, dtype=torch.complex128)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log2(torch.linalg.det(torch.eye(2, dtype=torch.cdouble) + H['0'] @ sigma @ H['0'].conj().T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea007e7",
   "metadata": {},
   "source": [
    "pga with V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c0ad2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fixed_channel_pga():\n",
    "\n",
    "    def __init__(self, H, PT):\n",
    "        self.H = H\n",
    "        self.PT = PT\n",
    "        self.Nr, self.Nt = H.shape\n",
    "        \n",
    "    def solve(self, num_iter=200000, lr=0.1):\n",
    "        def proj_psd_trace(V, P):\n",
    "            s = torch.trace(V @ V.conj().T)\n",
    "            return torch.sqrt(P / s)*V\n",
    "        \n",
    "        I = torch.eye(self.Nr, dtype=torch.cdouble)\n",
    "\n",
    "        V = torch.randn(self.Nt, 2, dtype=torch.cdouble, requires_grad=True)\n",
    "        # M = I + self.H @ V @ V.conj().T @ self.H.conj().T\n",
    "        # print(I + self.H @ V @ V.conj().T @ self.H.conj().T)\n",
    "        for _ in range(num_iter): \n",
    "            V.requires_grad_(True)\n",
    "            M = I + self.H @ V @ V.conj().T @ self.H.conj().T\n",
    "            # print(M)\n",
    "            loss = torch.logdet(M).real\n",
    "            loss.backward()\n",
    "            g = V.grad\n",
    "\n",
    "            with torch.no_grad():\n",
    "                V = V + lr * g\n",
    "                V = proj_psd_trace(V, self.PT)\n",
    "\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "179af9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pga = fixed_channel_pga(H['0'], 300)\n",
    "V = pga.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8ed57227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.6912+8.0642e-17j, dtype=torch.complex128)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log2(torch.linalg.det(torch.eye(2, dtype=torch.cdouble) + H['0'] @ V @ V.conj().T @ H['0'].conj().T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-Unfolding-NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
