{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f547db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43de5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_dual_link_torch(power_total: float, weight: torch.Tensor, H_links: torch.Tensor,\n",
    "                  rate_diff: float, Sigma=None, device=None):\n",
    "    \"\"\"\n",
    "    PyTorch version of dual-link algorithm\n",
    "    \n",
    "    Args:\n",
    "        power_total: Total power constraint\n",
    "        weight: Weight tensor of shape (n_links,)\n",
    "        H_links: Channel matrix tensor of shape (n_links, n_links, n_rx, n_tx)\n",
    "        rate_diff: Rate difference threshold for convergence\n",
    "        Sigma: Initial covariance matrices (optional)\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        sum_rate_list: List of sum rates per iteration\n",
    "        final_sum_rate: Final sum rate\n",
    "        rates: Individual link rates\n",
    "        Sigma: Final covariance matrices\n",
    "        Sigma_hat: Final dual covariance matrices\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = H_links.device\n",
    "    \n",
    "    # Move tensors to device\n",
    "    H_links = H_links.to(device)\n",
    "    weight = weight.to(device)\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    # find basic parameters\n",
    "    #------------------------------------------------\n",
    "    n_links = H_links.shape[0]\n",
    "    rates = torch.zeros(n_links, device=device)\n",
    "    Sigma_hat = [None] * n_links\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    # If initial Sigma not given, make it identity\n",
    "    #------------------------------------------------\n",
    "    if Sigma is None:\n",
    "        Sigma = [None] * n_links\n",
    "        for l_link in range(n_links):\n",
    "            lt_l = H_links[l_link, l_link].shape[1]\n",
    "            # power constraint may not be satisfied\n",
    "            Sigma[l_link] = (power_total / lt_l / n_links * \n",
    "                           torch.eye(lt_l, dtype=torch.complex64, device=device))\n",
    "    \n",
    "    #---------------------------------------------\n",
    "    # repeat until rate change is small\n",
    "    sum_rate_temp = torch.tensor(-float('inf'), device=device)\n",
    "    sum_rate_list = []\n",
    "    \n",
    "    num_epochs = 0\n",
    "    while num_epochs < 1:\n",
    "        num_epochs += 1\n",
    "        # calculate reverse link Sigma_hat's\n",
    "        power_normalizer = torch.tensor(0.0, device=device)\n",
    "        sum_rate = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        for l_link in range(n_links):\n",
    "            # calculate forward link interference\n",
    "            total_Cov_l = torch.eye(H_links[l_link, l_link].shape[0], \n",
    "                                  dtype=torch.complex64, device=device)\n",
    "            \n",
    "            for k_link in range(n_links):\n",
    "                total_Cov_l += (H_links[l_link, k_link] @ Sigma[k_link] @ \n",
    "                              H_links[l_link, k_link].T.conj())\n",
    "            \n",
    "            Omega_l = (total_Cov_l - H_links[l_link, l_link] @ Sigma[l_link] @ \n",
    "                      H_links[l_link, l_link].T.conj())\n",
    "            \n",
    "            # -------------------------------------------------------------------\n",
    "            # calculate the rates using log determinant\n",
    "            hy = torch.logdet(total_Cov_l).real\n",
    "            hy_x = torch.logdet(Omega_l).real\n",
    "            rates[l_link] = hy - hy_x\n",
    "            # -------------------------------------------------------------------\n",
    "            sum_rate += weight[l_link] * rates[l_link]\n",
    "            \n",
    "            Sigma_hat[l_link] = weight[l_link] * (torch.linalg.inv(Omega_l) -\n",
    "                                                torch.linalg.inv(total_Cov_l))\n",
    "            power_normalizer += torch.trace(Sigma_hat[l_link]).real\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # break out of loop if rate change is small\n",
    "        sum_rate_list.append((sum_rate / torch.log(torch.tensor(2.0))).item())\n",
    "        \n",
    "        # if (sum_rate / torch.log(torch.tensor(2.0)) - \n",
    "        #     sum_rate_temp / torch.log(torch.tensor(2.0))) < rate_diff:\n",
    "        #     break\n",
    "        \n",
    "        # --------------------------------------------------------------------\n",
    "        sum_rate_temp = sum_rate\n",
    "        \n",
    "        # Normalize Sigma_hat\n",
    "        for l_link in range(n_links):\n",
    "            Sigma_hat[l_link] = Sigma_hat[l_link] * power_total / power_normalizer\n",
    "        \n",
    "        # --------------------------------------------------------------------\n",
    "        # calculate forward link Sigma's\n",
    "        # --------------------------------------------------------------------\n",
    "        power_normalizer = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        for l_link in range(n_links):\n",
    "            # calculate forward link interference\n",
    "            total_Cov_hat_l = torch.eye(H_links[l_link, l_link].shape[1], \n",
    "                                      dtype=torch.complex64, device=device)\n",
    "            \n",
    "            for k_link in range(n_links):\n",
    "                total_Cov_hat_l += (H_links[k_link, l_link].T.conj() @ \n",
    "                                  Sigma_hat[k_link] @ H_links[k_link, l_link])\n",
    "            \n",
    "            Omega_hat_l = (total_Cov_hat_l - H_links[l_link, l_link].T.conj() @ \n",
    "                          Sigma_hat[l_link] @ H_links[l_link, l_link])\n",
    "            \n",
    "            Sigma[l_link] = weight[l_link] * (torch.linalg.inv(Omega_hat_l) -\n",
    "                                            torch.linalg.inv(total_Cov_hat_l))\n",
    "            power_normalizer += torch.trace(Sigma[l_link]).real\n",
    "        \n",
    "        # Normalize Sigma\n",
    "        for l_link in range(n_links):\n",
    "            Sigma[l_link] = Sigma[l_link] * power_total / power_normalizer\n",
    "    \n",
    "    return (sum_rate_list, \n",
    "            (sum_rate / torch.log(torch.tensor(2.0))),\n",
    "            (rates / torch.log(torch.tensor(2.0))),\n",
    "            Sigma, \n",
    "            Sigma_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99936cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelCNN(nn.Module):\n",
    "    def __init__(self, setup):\n",
    "        super(ChannelCNN, self).__init__()\n",
    "        self.L = setup.L\n",
    "        self.n_rx = setup.n_rx\n",
    "        self.n_tx = setup.n_tx\n",
    "        self.d = setup.d\n",
    "\n",
    "        in_channels = 2 * self.L * self.L\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * self.n_rx * self.n_tx, 256)\n",
    "        self.fc2 = nn.Linear(256, 2 * self.L * self.n_tx * self.d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Reshape to [B, L, 2, n_tx, d]\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, self.L, 2, self.n_tx, self.d)\n",
    "\n",
    "        real = x[:, :, 0, :, :]\n",
    "        imag = x[:, :, 1, :, :]\n",
    "        return torch.complex(real, imag)  # [B, L, n_tx, d]\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x)  # [B, L, n_tx, d]\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class ChannelCNNTrainer():\n",
    "    def __init__(self, model: ChannelCNN, setup, lr=1e-3):\n",
    "        self.model = model\n",
    "        self.setup = setup\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, train_list, dual_link_fn, num_epochs=100, batch_size=2):\n",
    "        self.model.train()\n",
    "        num_samples = len(train_list)\n",
    "\n",
    "        # def V_to_Sigma(V):\n",
    "        #     Sigma = torch.zeros_like(V)\n",
    "        #     for i in range(len(V)):\n",
    "        #         for j in range(self.setup.L):\n",
    "        #             Sigma[i][j] = V[i, j] @ V[i, j].conj().T\n",
    "        #     return Sigma\n",
    "\n",
    "        def V_to_Sigma(V):\n",
    "            \"\"\"Convert V matrices to Sigma covariance matrices\"\"\"\n",
    "            Sigma = []\n",
    "            for i in range(len(V)):\n",
    "                Sigma_i = []\n",
    "                for j in range(self.setup.L):\n",
    "                    # Create new tensor instead of modifying in-place\n",
    "                    sigma_j = V[i, j] @ V[i, j].conj().T\n",
    "                    Sigma_i.append(sigma_j)\n",
    "                Sigma.append(torch.stack(Sigma_i))\n",
    "            return torch.stack(Sigma)\n",
    "\n",
    "\n",
    "        # def proj_power(Sigma):\n",
    "        #     for i in range(len(Sigma)):\n",
    "        #         s = 0\n",
    "        #         for j in range(self.setup.L):\n",
    "        #             s += torch.trace(Sigma[i, j])\n",
    "        #         for j in range(self.setup.L):\n",
    "        #             Sigma[i, j] = (self.setup.PT/s) * Sigma[i, j]\n",
    "        #     return Sigma\n",
    "        def proj_power(Sigma):\n",
    "            \"\"\"Project Sigma matrices to satisfy power constraint\"\"\"\n",
    "            Sigma_proj = []\n",
    "            for i in range(len(Sigma)):\n",
    "                # Calculate total power for this sample\n",
    "                total_power = torch.tensor(0.0, dtype=torch.float32)\n",
    "                for j in range(self.setup.L):\n",
    "                    total_power = total_power + torch.trace(Sigma[i][j]).real\n",
    "                \n",
    "                # Create new normalized matrices (avoid in-place operations)\n",
    "                Sigma_proj_i = []\n",
    "                for j in range(self.setup.L):\n",
    "                    # Create new tensor instead of modifying in-place\n",
    "                    sigma_normalized = (self.setup.PT / total_power) * Sigma[i][j]\n",
    "                    Sigma_proj_i.append(sigma_normalized)\n",
    "                Sigma_proj.append(torch.stack(Sigma_proj_i))\n",
    "            return torch.stack(Sigma_proj)\n",
    "        \n",
    "        def sum_rate_loss(Sigma, H_list):\n",
    "            total = 0\n",
    "            for i in range(len(H_list)):\n",
    "                s_rate = 0\n",
    "                for l in range(H_list[0].shape[0]):\n",
    "                    Omeg = 0\n",
    "                    for k in range(H_list[0].shape[0]):\n",
    "                        if k != l:\n",
    "                            Omeg += H_list[i][l, k] @ Sigma[i][k] @ H_list[i][l, k].conj().T\n",
    "                    Omeg = torch.eye(H_list[0].shape[2]) + Omeg\n",
    "                    rate = torch.log2(torch.linalg.det(torch.eye(H_list[0].shape[2]) + H_list[i][l, l] @ Sigma[i][l] @ H_list[i][l, l].conj().T @ torch.linalg.inv(Omeg)))\n",
    "                    s_rate += rate.real\n",
    "                total += s_rate\n",
    "            return total/len(H_list)\n",
    "        \n",
    "        def sep_real_imag(x):\n",
    "            real = x.real\n",
    "            imag = x.imag\n",
    "            B, L, _, n_rx, n_tx = real.shape\n",
    "            real = real.view(B, L * L, n_rx, n_tx)\n",
    "            imag = imag.view(B, L * L, n_rx, n_tx)\n",
    "            x_prepared = torch.cat([real, imag], dim=1)\n",
    "            return x_prepared\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                batch_list = train_list[i:i + batch_size]\n",
    "                batch_tensor = torch.stack(batch_list)\n",
    "                batch_tensor_sep = sep_real_imag(batch_tensor)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                V_init = self.model.forward(batch_tensor_sep)\n",
    "\n",
    "                Sigma_init = V_to_Sigma(V_init)\n",
    "\n",
    "                Sigma_proj = proj_power(Sigma_init)\n",
    "\n",
    "                Sigma_final_list = []\n",
    "\n",
    "                for j in range(len(Sigma_proj)):\n",
    "                    _, _, _, Sigma_final, _ = dual_link_fn(power_total=self.setup.PT, weight=torch.ones(self.setup.L), H_links=batch_tensor[j], rate_diff=.001, Sigma=Sigma_proj[j], device=None)\n",
    "\n",
    "                    Sigma_final_list.append(Sigma_final)\n",
    "\n",
    "                loss = -1 * sum_rate_loss(Sigma_final_list, batch_list)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} | Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class setup():\n",
    "    def __init__(self, L, n_tx, n_rx, d, PT):\n",
    "        self.L = L\n",
    "        self.n_rx = n_rx\n",
    "        self.n_tx = n_tx\n",
    "        self.d = d\n",
    "        self.PT = PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac197168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: -6.3336\n",
      "Epoch 2 | Loss: -8.9640\n",
      "Epoch 3 | Loss: -11.8240\n",
      "Epoch 4 | Loss: -12.5363\n",
      "Epoch 5 | Loss: -13.1393\n",
      "Epoch 6 | Loss: -13.8673\n",
      "Epoch 7 | Loss: -14.7271\n",
      "Epoch 8 | Loss: -14.9656\n",
      "Epoch 9 | Loss: -15.4253\n",
      "Epoch 10 | Loss: -15.5965\n",
      "Epoch 11 | Loss: -15.6489\n",
      "Epoch 12 | Loss: -15.7284\n",
      "Epoch 13 | Loss: -15.7740\n",
      "Epoch 14 | Loss: -15.7921\n",
      "Epoch 15 | Loss: -15.8212\n",
      "Epoch 16 | Loss: -15.8641\n",
      "Epoch 17 | Loss: -15.9011\n",
      "Epoch 18 | Loss: -15.9208\n",
      "Epoch 19 | Loss: -15.9272\n",
      "Epoch 20 | Loss: -15.9269\n",
      "Epoch 21 | Loss: -15.9248\n",
      "Epoch 22 | Loss: -15.9232\n",
      "Epoch 23 | Loss: -15.9224\n",
      "Epoch 24 | Loss: -15.9211\n",
      "Epoch 25 | Loss: -15.9184\n",
      "Epoch 26 | Loss: -15.9158\n",
      "Epoch 27 | Loss: -15.9160\n",
      "Epoch 28 | Loss: -15.9198\n",
      "Epoch 29 | Loss: -15.9251\n",
      "Epoch 30 | Loss: -15.9286\n",
      "Epoch 31 | Loss: -15.9287\n",
      "Epoch 32 | Loss: -15.9269\n",
      "Epoch 33 | Loss: -15.9262\n",
      "Epoch 34 | Loss: -15.9283\n",
      "Epoch 35 | Loss: -15.9322\n",
      "Epoch 36 | Loss: -15.9357\n",
      "Epoch 37 | Loss: -15.9380\n",
      "Epoch 38 | Loss: -15.9392\n",
      "Epoch 39 | Loss: -15.9405\n",
      "Epoch 40 | Loss: -15.9418\n",
      "Epoch 41 | Loss: -15.9431\n",
      "Epoch 42 | Loss: -15.9442\n",
      "Epoch 43 | Loss: -15.9446\n",
      "Epoch 44 | Loss: -15.9447\n",
      "Epoch 45 | Loss: -15.9450\n",
      "Epoch 46 | Loss: -15.9458\n",
      "Epoch 47 | Loss: -15.9466\n",
      "Epoch 48 | Loss: -15.9472\n",
      "Epoch 49 | Loss: -15.9475\n",
      "Epoch 50 | Loss: -15.9477\n",
      "Epoch 51 | Loss: -15.9480\n",
      "Epoch 52 | Loss: -15.9482\n",
      "Epoch 53 | Loss: -15.9485\n",
      "Epoch 54 | Loss: -15.9490\n",
      "Epoch 55 | Loss: -15.9494\n",
      "Epoch 56 | Loss: -15.9500\n",
      "Epoch 57 | Loss: -15.9505\n",
      "Epoch 58 | Loss: -15.9511\n",
      "Epoch 59 | Loss: -15.9517\n",
      "Epoch 60 | Loss: -15.9522\n",
      "Epoch 61 | Loss: -15.9527\n",
      "Epoch 62 | Loss: -15.9530\n",
      "Epoch 63 | Loss: -15.9533\n",
      "Epoch 64 | Loss: -15.9535\n",
      "Epoch 65 | Loss: -15.9539\n",
      "Epoch 66 | Loss: -15.9542\n",
      "Epoch 67 | Loss: -15.9544\n",
      "Epoch 68 | Loss: -15.9548\n",
      "Epoch 69 | Loss: -15.9552\n",
      "Epoch 70 | Loss: -15.9556\n",
      "Epoch 71 | Loss: -15.9559\n",
      "Epoch 72 | Loss: -15.9561\n",
      "Epoch 73 | Loss: -15.9563\n",
      "Epoch 74 | Loss: -15.9565\n",
      "Epoch 75 | Loss: -15.9567\n",
      "Epoch 76 | Loss: -15.9570\n",
      "Epoch 77 | Loss: -15.9573\n",
      "Epoch 78 | Loss: -15.9576\n",
      "Epoch 79 | Loss: -15.9579\n",
      "Epoch 80 | Loss: -15.9580\n",
      "Epoch 81 | Loss: -15.9581\n",
      "Epoch 82 | Loss: -15.9582\n",
      "Epoch 83 | Loss: -15.9583\n",
      "Epoch 84 | Loss: -15.9584\n",
      "Epoch 85 | Loss: -15.9586\n",
      "Epoch 86 | Loss: -15.9586\n",
      "Epoch 87 | Loss: -15.9587\n",
      "Epoch 88 | Loss: -15.9588\n",
      "Epoch 89 | Loss: -15.9590\n",
      "Epoch 90 | Loss: -15.9591\n",
      "Epoch 91 | Loss: -15.9592\n",
      "Epoch 92 | Loss: -15.9593\n",
      "Epoch 93 | Loss: -15.9594\n",
      "Epoch 94 | Loss: -15.9595\n",
      "Epoch 95 | Loss: -15.9595\n",
      "Epoch 96 | Loss: -15.9596\n",
      "Epoch 97 | Loss: -15.9597\n",
      "Epoch 98 | Loss: -15.9596\n",
      "Epoch 99 | Loss: -15.9595\n",
      "Epoch 100 | Loss: -15.9598\n",
      "Epoch 101 | Loss: -15.9597\n",
      "Epoch 102 | Loss: -15.9597\n",
      "Epoch 103 | Loss: -15.9598\n",
      "Epoch 104 | Loss: -15.9598\n",
      "Epoch 105 | Loss: -15.9599\n",
      "Epoch 106 | Loss: -15.9599\n",
      "Epoch 107 | Loss: -15.9600\n",
      "Epoch 108 | Loss: -15.9599\n",
      "Epoch 109 | Loss: -15.9600\n",
      "Epoch 110 | Loss: -15.9599\n",
      "Epoch 111 | Loss: -15.9600\n",
      "Epoch 112 | Loss: -15.9600\n",
      "Epoch 113 | Loss: -15.9601\n",
      "Epoch 114 | Loss: -15.9600\n",
      "Epoch 115 | Loss: -15.9600\n",
      "Epoch 116 | Loss: -15.9600\n",
      "Epoch 117 | Loss: -15.9601\n",
      "Epoch 118 | Loss: -15.9601\n",
      "Epoch 119 | Loss: -15.9601\n",
      "Epoch 120 | Loss: -15.9601\n",
      "Epoch 121 | Loss: -15.9601\n",
      "Epoch 122 | Loss: -15.9601\n",
      "Epoch 123 | Loss: -15.9601\n",
      "Epoch 124 | Loss: -15.9601\n",
      "Epoch 125 | Loss: -15.9601\n",
      "Epoch 126 | Loss: -15.9601\n",
      "Epoch 127 | Loss: -15.9601\n",
      "Epoch 128 | Loss: -15.9601\n",
      "Epoch 129 | Loss: -15.9601\n",
      "Epoch 130 | Loss: -15.9601\n",
      "Epoch 131 | Loss: -15.9601\n",
      "Epoch 132 | Loss: -15.9601\n",
      "Epoch 133 | Loss: -15.9601\n",
      "Epoch 134 | Loss: -15.9601\n",
      "Epoch 135 | Loss: -15.9601\n",
      "Epoch 136 | Loss: -15.9601\n",
      "Epoch 137 | Loss: -15.9602\n",
      "Epoch 138 | Loss: -15.9601\n",
      "Epoch 139 | Loss: -15.9601\n",
      "Epoch 140 | Loss: -15.9601\n",
      "Epoch 141 | Loss: -15.9601\n",
      "Epoch 142 | Loss: -15.9601\n",
      "Epoch 143 | Loss: -15.9601\n",
      "Epoch 144 | Loss: -15.9601\n",
      "Epoch 145 | Loss: -15.9601\n",
      "Epoch 146 | Loss: -15.9601\n",
      "Epoch 147 | Loss: -15.9601\n",
      "Epoch 148 | Loss: -15.9601\n",
      "Epoch 149 | Loss: -15.9602\n",
      "Epoch 150 | Loss: -15.9601\n",
      "Epoch 151 | Loss: -15.9601\n",
      "Epoch 152 | Loss: -15.9601\n",
      "Epoch 153 | Loss: -15.9602\n",
      "Epoch 154 | Loss: -15.9601\n",
      "Epoch 155 | Loss: -15.9602\n",
      "Epoch 156 | Loss: -15.9602\n",
      "Epoch 157 | Loss: -15.9601\n",
      "Epoch 158 | Loss: -15.9602\n",
      "Epoch 159 | Loss: -15.9602\n",
      "Epoch 160 | Loss: -15.9602\n",
      "Epoch 161 | Loss: -15.9602\n",
      "Epoch 162 | Loss: -15.9602\n",
      "Epoch 163 | Loss: -15.9601\n",
      "Epoch 164 | Loss: -15.9601\n",
      "Epoch 165 | Loss: -15.9602\n",
      "Epoch 166 | Loss: -15.9602\n",
      "Epoch 167 | Loss: -15.9602\n",
      "Epoch 168 | Loss: -15.9602\n",
      "Epoch 169 | Loss: -15.9601\n",
      "Epoch 170 | Loss: -15.9602\n",
      "Epoch 171 | Loss: -15.9602\n",
      "Epoch 172 | Loss: -15.9602\n",
      "Epoch 173 | Loss: -15.9601\n",
      "Epoch 174 | Loss: -15.9602\n",
      "Epoch 175 | Loss: -15.9602\n",
      "Epoch 176 | Loss: -15.9602\n",
      "Epoch 177 | Loss: -15.9602\n",
      "Epoch 178 | Loss: -15.9602\n",
      "Epoch 179 | Loss: -15.9602\n",
      "Epoch 180 | Loss: -15.9601\n",
      "Epoch 181 | Loss: -15.9602\n",
      "Epoch 182 | Loss: -15.9602\n",
      "Epoch 183 | Loss: -15.9602\n",
      "Epoch 184 | Loss: -15.9602\n",
      "Epoch 185 | Loss: -15.9602\n",
      "Epoch 186 | Loss: -15.9602\n",
      "Epoch 187 | Loss: -15.9602\n",
      "Epoch 188 | Loss: -15.9602\n",
      "Epoch 189 | Loss: -15.9602\n",
      "Epoch 190 | Loss: -15.9602\n",
      "Epoch 191 | Loss: -15.9602\n",
      "Epoch 192 | Loss: -15.9602\n",
      "Epoch 193 | Loss: -15.9602\n",
      "Epoch 194 | Loss: -15.9602\n",
      "Epoch 195 | Loss: -15.9602\n",
      "Epoch 196 | Loss: -15.9602\n",
      "Epoch 197 | Loss: -15.9602\n",
      "Epoch 198 | Loss: -15.9602\n",
      "Epoch 199 | Loss: -15.9602\n",
      "Epoch 200 | Loss: -15.9602\n",
      "Epoch 201 | Loss: -15.9602\n",
      "Epoch 202 | Loss: -15.9602\n",
      "Epoch 203 | Loss: -15.9602\n",
      "Epoch 204 | Loss: -15.9602\n",
      "Epoch 205 | Loss: -15.9602\n",
      "Epoch 206 | Loss: -15.9602\n",
      "Epoch 207 | Loss: -15.9602\n",
      "Epoch 208 | Loss: -15.9602\n",
      "Epoch 209 | Loss: -15.9602\n",
      "Epoch 210 | Loss: -15.9602\n",
      "Epoch 211 | Loss: -15.9602\n",
      "Epoch 212 | Loss: -15.9602\n",
      "Epoch 213 | Loss: -15.9602\n",
      "Epoch 214 | Loss: -15.9602\n",
      "Epoch 215 | Loss: -15.9602\n",
      "Epoch 216 | Loss: -15.9602\n",
      "Epoch 217 | Loss: -15.9602\n",
      "Epoch 218 | Loss: -15.9602\n",
      "Epoch 219 | Loss: -15.9602\n",
      "Epoch 220 | Loss: -15.9602\n",
      "Epoch 221 | Loss: -15.9602\n",
      "Epoch 222 | Loss: -15.9602\n",
      "Epoch 223 | Loss: -15.9602\n",
      "Epoch 224 | Loss: -15.9602\n",
      "Epoch 225 | Loss: -15.9602\n",
      "Epoch 226 | Loss: -15.9602\n",
      "Epoch 227 | Loss: -15.9602\n",
      "Epoch 228 | Loss: -15.9602\n",
      "Epoch 229 | Loss: -15.9602\n",
      "Epoch 230 | Loss: -15.9602\n",
      "Epoch 231 | Loss: -15.9602\n",
      "Epoch 232 | Loss: -15.9602\n",
      "Epoch 233 | Loss: -15.9602\n",
      "Epoch 234 | Loss: -15.9602\n",
      "Epoch 235 | Loss: -15.9602\n",
      "Epoch 236 | Loss: -15.9602\n",
      "Epoch 237 | Loss: -15.9602\n",
      "Epoch 238 | Loss: -15.9602\n",
      "Epoch 239 | Loss: -15.9602\n",
      "Epoch 240 | Loss: -15.9602\n",
      "Epoch 241 | Loss: -15.9602\n",
      "Epoch 242 | Loss: -15.9602\n",
      "Epoch 243 | Loss: -15.9602\n",
      "Epoch 244 | Loss: -15.9602\n",
      "Epoch 245 | Loss: -15.9602\n",
      "Epoch 246 | Loss: -15.9602\n",
      "Epoch 247 | Loss: -15.9602\n",
      "Epoch 248 | Loss: -15.9602\n",
      "Epoch 249 | Loss: -15.9602\n",
      "Epoch 250 | Loss: -15.9602\n",
      "Epoch 251 | Loss: -15.9602\n",
      "Epoch 252 | Loss: -15.9601\n",
      "Epoch 253 | Loss: -15.9601\n",
      "Epoch 254 | Loss: -15.9601\n",
      "Epoch 255 | Loss: -15.9601\n",
      "Epoch 256 | Loss: -15.9601\n",
      "Epoch 257 | Loss: -15.9602\n",
      "Epoch 258 | Loss: -15.9602\n",
      "Epoch 259 | Loss: -15.9602\n",
      "Epoch 260 | Loss: -15.9602\n",
      "Epoch 261 | Loss: -15.9602\n",
      "Epoch 262 | Loss: -15.9602\n",
      "Epoch 263 | Loss: -15.9602\n",
      "Epoch 264 | Loss: -15.9602\n",
      "Epoch 265 | Loss: -15.9602\n",
      "Epoch 266 | Loss: -15.9602\n",
      "Epoch 267 | Loss: -15.9602\n",
      "Epoch 268 | Loss: -15.9602\n",
      "Epoch 269 | Loss: -15.9602\n",
      "Epoch 270 | Loss: -15.9602\n",
      "Epoch 271 | Loss: -15.9602\n",
      "Epoch 272 | Loss: -15.9602\n",
      "Epoch 273 | Loss: -15.9602\n",
      "Epoch 274 | Loss: -15.9602\n",
      "Epoch 275 | Loss: -15.9602\n",
      "Epoch 276 | Loss: -15.9602\n",
      "Epoch 277 | Loss: -15.9602\n",
      "Epoch 278 | Loss: -15.9602\n",
      "Epoch 279 | Loss: -15.9602\n",
      "Epoch 280 | Loss: -15.9602\n",
      "Epoch 281 | Loss: -15.9602\n",
      "Epoch 282 | Loss: -15.9602\n",
      "Epoch 283 | Loss: -15.9602\n",
      "Epoch 284 | Loss: -15.9602\n",
      "Epoch 285 | Loss: -15.9602\n",
      "Epoch 286 | Loss: -15.9602\n",
      "Epoch 287 | Loss: -15.9602\n",
      "Epoch 288 | Loss: -15.9602\n",
      "Epoch 289 | Loss: -15.9602\n",
      "Epoch 290 | Loss: -15.9602\n",
      "Epoch 291 | Loss: -15.9602\n",
      "Epoch 292 | Loss: -15.9602\n",
      "Epoch 293 | Loss: -15.9602\n",
      "Epoch 294 | Loss: -15.9602\n",
      "Epoch 295 | Loss: -15.9602\n",
      "Epoch 296 | Loss: -15.9602\n",
      "Epoch 297 | Loss: -15.9602\n",
      "Epoch 298 | Loss: -15.9602\n",
      "Epoch 299 | Loss: -15.9602\n",
      "Epoch 300 | Loss: -15.9602\n",
      "Epoch 301 | Loss: -15.9602\n",
      "Epoch 302 | Loss: -15.9602\n",
      "Epoch 303 | Loss: -15.9602\n",
      "Epoch 304 | Loss: -15.9602\n",
      "Epoch 305 | Loss: -15.9602\n",
      "Epoch 306 | Loss: -15.9602\n",
      "Epoch 307 | Loss: -15.9602\n",
      "Epoch 308 | Loss: -15.9602\n",
      "Epoch 309 | Loss: -15.9602\n",
      "Epoch 310 | Loss: -15.9602\n",
      "Epoch 311 | Loss: -15.9602\n",
      "Epoch 312 | Loss: -15.9602\n",
      "Epoch 313 | Loss: -15.9602\n",
      "Epoch 314 | Loss: -15.9602\n",
      "Epoch 315 | Loss: -15.9602\n",
      "Epoch 316 | Loss: -15.9602\n",
      "Epoch 317 | Loss: -15.9602\n",
      "Epoch 318 | Loss: -15.9602\n",
      "Epoch 319 | Loss: -15.9602\n",
      "Epoch 320 | Loss: -15.9602\n",
      "Epoch 321 | Loss: -15.9602\n",
      "Epoch 322 | Loss: -15.9602\n",
      "Epoch 323 | Loss: -15.9602\n",
      "Epoch 324 | Loss: -15.9602\n",
      "Epoch 325 | Loss: -15.9602\n",
      "Epoch 326 | Loss: -15.9602\n",
      "Epoch 327 | Loss: -15.9602\n",
      "Epoch 328 | Loss: -15.9601\n",
      "Epoch 329 | Loss: -15.9601\n",
      "Epoch 330 | Loss: -15.9600\n",
      "Epoch 331 | Loss: -15.9600\n",
      "Epoch 332 | Loss: -15.9600\n",
      "Epoch 333 | Loss: -15.9601\n",
      "Epoch 334 | Loss: -15.9602\n",
      "Epoch 335 | Loss: -15.9602\n",
      "Epoch 336 | Loss: -15.9601\n",
      "Epoch 337 | Loss: -15.9601\n",
      "Epoch 338 | Loss: -15.9602\n",
      "Epoch 339 | Loss: -15.9602\n",
      "Epoch 340 | Loss: -15.9601\n",
      "Epoch 341 | Loss: -15.9601\n",
      "Epoch 342 | Loss: -15.9602\n",
      "Epoch 343 | Loss: -15.9602\n",
      "Epoch 344 | Loss: -15.9602\n",
      "Epoch 345 | Loss: -15.9602\n",
      "Epoch 346 | Loss: -15.9602\n",
      "Epoch 347 | Loss: -15.9602\n",
      "Epoch 348 | Loss: -15.9602\n",
      "Epoch 349 | Loss: -15.9602\n",
      "Epoch 350 | Loss: -15.9602\n",
      "Epoch 351 | Loss: -15.9602\n",
      "Epoch 352 | Loss: -15.9602\n",
      "Epoch 353 | Loss: -15.9602\n",
      "Epoch 354 | Loss: -15.9602\n",
      "Epoch 355 | Loss: -15.9602\n",
      "Epoch 356 | Loss: -15.9602\n",
      "Epoch 357 | Loss: -15.9602\n",
      "Epoch 358 | Loss: -15.9602\n",
      "Epoch 359 | Loss: -15.9602\n",
      "Epoch 360 | Loss: -15.9602\n",
      "Epoch 361 | Loss: -15.9602\n",
      "Epoch 362 | Loss: -15.9602\n",
      "Epoch 363 | Loss: -15.9602\n",
      "Epoch 364 | Loss: -15.9602\n",
      "Epoch 365 | Loss: -15.9602\n",
      "Epoch 366 | Loss: -15.9602\n",
      "Epoch 367 | Loss: -15.9602\n",
      "Epoch 368 | Loss: -15.9602\n",
      "Epoch 369 | Loss: -15.9602\n",
      "Epoch 370 | Loss: -15.9602\n",
      "Epoch 371 | Loss: -15.9602\n",
      "Epoch 372 | Loss: -15.9602\n",
      "Epoch 373 | Loss: -15.9602\n",
      "Epoch 374 | Loss: -15.9602\n",
      "Epoch 375 | Loss: -15.9602\n",
      "Epoch 376 | Loss: -15.9602\n",
      "Epoch 377 | Loss: -15.9602\n",
      "Epoch 378 | Loss: -15.9602\n",
      "Epoch 379 | Loss: -15.9602\n",
      "Epoch 380 | Loss: -15.9602\n",
      "Epoch 381 | Loss: -15.9602\n",
      "Epoch 382 | Loss: -15.9602\n",
      "Epoch 383 | Loss: -15.9602\n",
      "Epoch 384 | Loss: -15.9602\n",
      "Epoch 385 | Loss: -15.9602\n",
      "Epoch 386 | Loss: -15.9602\n",
      "Epoch 387 | Loss: -15.9602\n",
      "Epoch 388 | Loss: -15.9602\n",
      "Epoch 389 | Loss: -15.9602\n",
      "Epoch 390 | Loss: -15.9602\n",
      "Epoch 391 | Loss: -15.9602\n",
      "Epoch 392 | Loss: -15.9602\n",
      "Epoch 393 | Loss: -15.9602\n",
      "Epoch 394 | Loss: -15.9602\n",
      "Epoch 395 | Loss: -15.9602\n",
      "Epoch 396 | Loss: -15.9602\n",
      "Epoch 397 | Loss: -15.9602\n",
      "Epoch 398 | Loss: -15.9602\n",
      "Epoch 399 | Loss: -15.9602\n",
      "Epoch 400 | Loss: -15.9602\n",
      "Epoch 401 | Loss: -15.9602\n",
      "Epoch 402 | Loss: -15.9602\n",
      "Epoch 403 | Loss: -15.9602\n",
      "Epoch 404 | Loss: -15.9602\n",
      "Epoch 405 | Loss: -15.9602\n",
      "Epoch 406 | Loss: -15.9602\n",
      "Epoch 407 | Loss: -15.9602\n",
      "Epoch 408 | Loss: -15.9602\n",
      "Epoch 409 | Loss: -15.9602\n",
      "Epoch 410 | Loss: -15.9602\n",
      "Epoch 411 | Loss: -15.9602\n",
      "Epoch 412 | Loss: -15.9602\n",
      "Epoch 413 | Loss: -15.9602\n",
      "Epoch 414 | Loss: -15.9602\n",
      "Epoch 415 | Loss: -15.9602\n",
      "Epoch 416 | Loss: -15.9602\n",
      "Epoch 417 | Loss: -15.9602\n",
      "Epoch 418 | Loss: -15.9602\n",
      "Epoch 419 | Loss: -15.9602\n",
      "Epoch 420 | Loss: -15.9602\n",
      "Epoch 421 | Loss: -15.9602\n",
      "Epoch 422 | Loss: -15.9602\n",
      "Epoch 423 | Loss: -15.9602\n",
      "Epoch 424 | Loss: -15.9602\n",
      "Epoch 425 | Loss: -15.9602\n",
      "Epoch 426 | Loss: -15.9602\n",
      "Epoch 427 | Loss: -15.9602\n",
      "Epoch 428 | Loss: -15.9602\n",
      "Epoch 429 | Loss: -15.9602\n",
      "Epoch 430 | Loss: -15.9602\n",
      "Epoch 431 | Loss: -15.9602\n",
      "Epoch 432 | Loss: -15.9602\n",
      "Epoch 433 | Loss: -15.9602\n",
      "Epoch 434 | Loss: -15.9602\n",
      "Epoch 435 | Loss: -15.9602\n",
      "Epoch 436 | Loss: -15.9602\n",
      "Epoch 437 | Loss: -15.9602\n",
      "Epoch 438 | Loss: -15.9602\n",
      "Epoch 439 | Loss: -15.9602\n",
      "Epoch 440 | Loss: -15.9602\n",
      "Epoch 441 | Loss: -15.9602\n",
      "Epoch 442 | Loss: -15.9602\n",
      "Epoch 443 | Loss: -15.9602\n",
      "Epoch 444 | Loss: -15.9602\n",
      "Epoch 445 | Loss: -15.9602\n",
      "Epoch 446 | Loss: -15.9602\n",
      "Epoch 447 | Loss: -15.9602\n",
      "Epoch 448 | Loss: -15.9602\n",
      "Epoch 449 | Loss: -15.9602\n",
      "Epoch 450 | Loss: -15.9602\n",
      "Epoch 451 | Loss: -15.9602\n",
      "Epoch 452 | Loss: -15.9602\n",
      "Epoch 453 | Loss: -15.9602\n",
      "Epoch 454 | Loss: -15.9602\n",
      "Epoch 455 | Loss: -15.9602\n",
      "Epoch 456 | Loss: -15.9602\n",
      "Epoch 457 | Loss: -15.9602\n",
      "Epoch 458 | Loss: -15.9602\n",
      "Epoch 459 | Loss: -15.9602\n",
      "Epoch 460 | Loss: -15.9602\n",
      "Epoch 461 | Loss: -15.9602\n",
      "Epoch 462 | Loss: -15.9602\n",
      "Epoch 463 | Loss: -15.9602\n",
      "Epoch 464 | Loss: -15.9602\n",
      "Epoch 465 | Loss: -15.9602\n",
      "Epoch 466 | Loss: -15.9602\n",
      "Epoch 467 | Loss: -15.9602\n",
      "Epoch 468 | Loss: -15.9602\n",
      "Epoch 469 | Loss: -15.9602\n",
      "Epoch 470 | Loss: -15.9602\n",
      "Epoch 471 | Loss: -15.9602\n",
      "Epoch 472 | Loss: -15.9602\n",
      "Epoch 473 | Loss: -15.9602\n",
      "Epoch 474 | Loss: -15.9602\n",
      "Epoch 475 | Loss: -15.9602\n",
      "Epoch 476 | Loss: -15.9602\n",
      "Epoch 477 | Loss: -15.9602\n",
      "Epoch 478 | Loss: -15.9602\n",
      "Epoch 479 | Loss: -15.9602\n",
      "Epoch 480 | Loss: -15.9602\n",
      "Epoch 481 | Loss: -15.9602\n",
      "Epoch 482 | Loss: -15.9602\n",
      "Epoch 483 | Loss: -15.9602\n",
      "Epoch 484 | Loss: -15.9602\n",
      "Epoch 485 | Loss: -15.9602\n",
      "Epoch 486 | Loss: -15.9602\n",
      "Epoch 487 | Loss: -15.9602\n",
      "Epoch 488 | Loss: -15.9602\n",
      "Epoch 489 | Loss: -15.9602\n",
      "Epoch 490 | Loss: -15.9602\n",
      "Epoch 491 | Loss: -15.9602\n",
      "Epoch 492 | Loss: -15.9602\n",
      "Epoch 493 | Loss: -15.9602\n",
      "Epoch 494 | Loss: -15.9602\n",
      "Epoch 495 | Loss: -15.9602\n",
      "Epoch 496 | Loss: -15.9602\n",
      "Epoch 497 | Loss: -15.9602\n",
      "Epoch 498 | Loss: -15.9602\n",
      "Epoch 499 | Loss: -15.9602\n",
      "Epoch 500 | Loss: -15.9602\n",
      "Epoch 501 | Loss: -15.9602\n",
      "Epoch 502 | Loss: -15.9602\n",
      "Epoch 503 | Loss: -15.9602\n",
      "Epoch 504 | Loss: -15.9601\n",
      "Epoch 505 | Loss: -15.9601\n",
      "Epoch 506 | Loss: -15.9601\n",
      "Epoch 507 | Loss: -15.9601\n",
      "Epoch 508 | Loss: -15.9601\n",
      "Epoch 509 | Loss: -15.9602\n",
      "Epoch 510 | Loss: -15.9602\n",
      "Epoch 511 | Loss: -15.9602\n",
      "Epoch 512 | Loss: -15.9602\n",
      "Epoch 513 | Loss: -15.9602\n",
      "Epoch 514 | Loss: -15.9602\n",
      "Epoch 515 | Loss: -15.9602\n",
      "Epoch 516 | Loss: -15.9602\n",
      "Epoch 517 | Loss: -15.9602\n",
      "Epoch 518 | Loss: -15.9602\n",
      "Epoch 519 | Loss: -15.9602\n",
      "Epoch 520 | Loss: -15.9602\n",
      "Epoch 521 | Loss: -15.9602\n",
      "Epoch 522 | Loss: -15.9602\n",
      "Epoch 523 | Loss: -15.9602\n",
      "Epoch 524 | Loss: -15.9602\n",
      "Epoch 525 | Loss: -15.9602\n",
      "Epoch 526 | Loss: -15.9602\n",
      "Epoch 527 | Loss: -15.9602\n",
      "Epoch 528 | Loss: -15.9602\n",
      "Epoch 529 | Loss: -15.9602\n",
      "Epoch 530 | Loss: -15.9602\n",
      "Epoch 531 | Loss: -15.9602\n",
      "Epoch 532 | Loss: -15.9602\n",
      "Epoch 533 | Loss: -15.9602\n",
      "Epoch 534 | Loss: -15.9602\n",
      "Epoch 535 | Loss: -15.9602\n",
      "Epoch 536 | Loss: -15.9602\n",
      "Epoch 537 | Loss: -15.9602\n",
      "Epoch 538 | Loss: -15.9602\n",
      "Epoch 539 | Loss: -15.9602\n",
      "Epoch 540 | Loss: -15.9602\n",
      "Epoch 541 | Loss: -15.9602\n",
      "Epoch 542 | Loss: -15.9602\n",
      "Epoch 543 | Loss: -15.9602\n",
      "Epoch 544 | Loss: -15.9602\n",
      "Epoch 545 | Loss: -15.9602\n",
      "Epoch 546 | Loss: -15.9602\n",
      "Epoch 547 | Loss: -15.9602\n",
      "Epoch 548 | Loss: -15.9602\n",
      "Epoch 549 | Loss: -15.9602\n",
      "Epoch 550 | Loss: -15.9602\n",
      "Epoch 551 | Loss: -15.9602\n",
      "Epoch 552 | Loss: -15.9602\n",
      "Epoch 553 | Loss: -15.9602\n",
      "Epoch 554 | Loss: -15.9602\n",
      "Epoch 555 | Loss: -15.9602\n",
      "Epoch 556 | Loss: -15.9602\n",
      "Epoch 557 | Loss: -15.9602\n",
      "Epoch 558 | Loss: -15.9602\n",
      "Epoch 559 | Loss: -15.9602\n",
      "Epoch 560 | Loss: -15.9602\n",
      "Epoch 561 | Loss: -15.9602\n",
      "Epoch 562 | Loss: -15.9602\n",
      "Epoch 563 | Loss: -15.9602\n",
      "Epoch 564 | Loss: -15.9602\n",
      "Epoch 565 | Loss: -15.9602\n",
      "Epoch 566 | Loss: -15.9602\n",
      "Epoch 567 | Loss: -15.9602\n",
      "Epoch 568 | Loss: -15.9602\n",
      "Epoch 569 | Loss: -15.9602\n",
      "Epoch 570 | Loss: -15.9602\n",
      "Epoch 571 | Loss: -15.9602\n",
      "Epoch 572 | Loss: -15.9602\n",
      "Epoch 573 | Loss: -15.9602\n",
      "Epoch 574 | Loss: -15.9602\n",
      "Epoch 575 | Loss: -15.9602\n",
      "Epoch 576 | Loss: -15.9602\n",
      "Epoch 577 | Loss: -15.9602\n",
      "Epoch 578 | Loss: -15.9602\n",
      "Epoch 579 | Loss: -15.9602\n",
      "Epoch 580 | Loss: -15.9602\n",
      "Epoch 581 | Loss: -15.9602\n",
      "Epoch 582 | Loss: -15.9602\n",
      "Epoch 583 | Loss: -15.9602\n",
      "Epoch 584 | Loss: -15.9602\n",
      "Epoch 585 | Loss: -15.9602\n",
      "Epoch 586 | Loss: -15.9602\n",
      "Epoch 587 | Loss: -15.9602\n",
      "Epoch 588 | Loss: -15.9602\n",
      "Epoch 589 | Loss: -15.9602\n",
      "Epoch 590 | Loss: -15.9602\n",
      "Epoch 591 | Loss: -15.9602\n",
      "Epoch 592 | Loss: -15.9602\n",
      "Epoch 593 | Loss: -15.9602\n",
      "Epoch 594 | Loss: -15.9602\n",
      "Epoch 595 | Loss: -15.9602\n",
      "Epoch 596 | Loss: -15.9602\n",
      "Epoch 597 | Loss: -15.9602\n",
      "Epoch 598 | Loss: -15.9602\n",
      "Epoch 599 | Loss: -15.9602\n",
      "Epoch 600 | Loss: -15.9602\n",
      "Epoch 601 | Loss: -15.9602\n",
      "Epoch 602 | Loss: -15.9602\n",
      "Epoch 603 | Loss: -15.9602\n",
      "Epoch 604 | Loss: -15.9602\n",
      "Epoch 605 | Loss: -15.9602\n",
      "Epoch 606 | Loss: -15.9602\n",
      "Epoch 607 | Loss: -15.9602\n",
      "Epoch 608 | Loss: -15.9602\n",
      "Epoch 609 | Loss: -15.9602\n",
      "Epoch 610 | Loss: -15.9602\n",
      "Epoch 611 | Loss: -15.9602\n",
      "Epoch 612 | Loss: -15.9602\n",
      "Epoch 613 | Loss: -15.9602\n",
      "Epoch 614 | Loss: -15.9602\n",
      "Epoch 615 | Loss: -15.9602\n",
      "Epoch 616 | Loss: -15.9602\n",
      "Epoch 617 | Loss: -15.9602\n",
      "Epoch 618 | Loss: -15.9602\n",
      "Epoch 619 | Loss: -15.9602\n",
      "Epoch 620 | Loss: -15.9602\n",
      "Epoch 621 | Loss: -15.9602\n",
      "Epoch 622 | Loss: -15.9602\n",
      "Epoch 623 | Loss: -15.9602\n",
      "Epoch 624 | Loss: -15.9602\n",
      "Epoch 625 | Loss: -15.9602\n",
      "Epoch 626 | Loss: -15.9602\n",
      "Epoch 627 | Loss: -15.9602\n",
      "Epoch 628 | Loss: -15.9602\n",
      "Epoch 629 | Loss: -15.9603\n",
      "Epoch 630 | Loss: -15.9602\n",
      "Epoch 631 | Loss: -15.9602\n",
      "Epoch 632 | Loss: -15.9602\n",
      "Epoch 633 | Loss: -15.9602\n",
      "Epoch 634 | Loss: -15.9602\n",
      "Epoch 635 | Loss: -15.9602\n",
      "Epoch 636 | Loss: -15.9602\n",
      "Epoch 637 | Loss: -15.9602\n",
      "Epoch 638 | Loss: -15.9602\n",
      "Epoch 639 | Loss: -15.9602\n",
      "Epoch 640 | Loss: -15.9602\n",
      "Epoch 641 | Loss: -15.9601\n",
      "Epoch 642 | Loss: -15.9601\n",
      "Epoch 643 | Loss: -15.9601\n",
      "Epoch 644 | Loss: -15.9602\n",
      "Epoch 645 | Loss: -15.9602\n",
      "Epoch 646 | Loss: -15.9602\n",
      "Epoch 647 | Loss: -15.9602\n",
      "Epoch 648 | Loss: -15.9602\n",
      "Epoch 649 | Loss: -15.9602\n",
      "Epoch 650 | Loss: -15.9602\n",
      "Epoch 651 | Loss: -15.9602\n",
      "Epoch 652 | Loss: -15.9602\n",
      "Epoch 653 | Loss: -15.9602\n",
      "Epoch 654 | Loss: -15.9602\n",
      "Epoch 655 | Loss: -15.9602\n",
      "Epoch 656 | Loss: -15.9602\n",
      "Epoch 657 | Loss: -15.9602\n",
      "Epoch 658 | Loss: -15.9602\n",
      "Epoch 659 | Loss: -15.9602\n",
      "Epoch 660 | Loss: -15.9602\n",
      "Epoch 661 | Loss: -15.9602\n",
      "Epoch 662 | Loss: -15.9602\n",
      "Epoch 663 | Loss: -15.9602\n",
      "Epoch 664 | Loss: -15.9602\n",
      "Epoch 665 | Loss: -15.9602\n",
      "Epoch 666 | Loss: -15.9602\n",
      "Epoch 667 | Loss: -15.9602\n",
      "Epoch 668 | Loss: -15.9602\n",
      "Epoch 669 | Loss: -15.9602\n",
      "Epoch 670 | Loss: -15.9602\n",
      "Epoch 671 | Loss: -15.9602\n",
      "Epoch 672 | Loss: -15.9602\n",
      "Epoch 673 | Loss: -15.9602\n",
      "Epoch 674 | Loss: -15.9602\n",
      "Epoch 675 | Loss: -15.9602\n",
      "Epoch 676 | Loss: -15.9602\n",
      "Epoch 677 | Loss: -15.9602\n",
      "Epoch 678 | Loss: -15.9602\n",
      "Epoch 679 | Loss: -15.9602\n",
      "Epoch 680 | Loss: -15.9602\n",
      "Epoch 681 | Loss: -15.9602\n",
      "Epoch 682 | Loss: -15.9603\n",
      "Epoch 683 | Loss: -15.9602\n",
      "Epoch 684 | Loss: -15.9602\n",
      "Epoch 685 | Loss: -15.9602\n",
      "Epoch 686 | Loss: -15.9602\n",
      "Epoch 687 | Loss: -15.9602\n",
      "Epoch 688 | Loss: -15.9602\n",
      "Epoch 689 | Loss: -15.9602\n",
      "Epoch 690 | Loss: -15.9602\n",
      "Epoch 691 | Loss: -15.9602\n",
      "Epoch 692 | Loss: -15.9602\n",
      "Epoch 693 | Loss: -15.9603\n",
      "Epoch 694 | Loss: -15.9602\n",
      "Epoch 695 | Loss: -15.9602\n",
      "Epoch 696 | Loss: -15.9602\n",
      "Epoch 697 | Loss: -15.9602\n",
      "Epoch 698 | Loss: -15.9602\n",
      "Epoch 699 | Loss: -15.9602\n",
      "Epoch 700 | Loss: -15.9602\n",
      "Epoch 701 | Loss: -15.9603\n",
      "Epoch 702 | Loss: -15.9602\n",
      "Epoch 703 | Loss: -15.9602\n",
      "Epoch 704 | Loss: -15.9602\n",
      "Epoch 705 | Loss: -15.9602\n",
      "Epoch 706 | Loss: -15.9603\n",
      "Epoch 707 | Loss: -15.9602\n",
      "Epoch 708 | Loss: -15.9603\n",
      "Epoch 709 | Loss: -15.9602\n",
      "Epoch 710 | Loss: -15.9602\n",
      "Epoch 711 | Loss: -15.9602\n",
      "Epoch 712 | Loss: -15.9603\n",
      "Epoch 713 | Loss: -15.9602\n",
      "Epoch 714 | Loss: -15.9602\n",
      "Epoch 715 | Loss: -15.9602\n",
      "Epoch 716 | Loss: -15.9602\n",
      "Epoch 717 | Loss: -15.9602\n",
      "Epoch 718 | Loss: -15.9602\n",
      "Epoch 719 | Loss: -15.9602\n",
      "Epoch 720 | Loss: -15.9602\n",
      "Epoch 721 | Loss: -15.9602\n",
      "Epoch 722 | Loss: -15.9602\n",
      "Epoch 723 | Loss: -15.9602\n",
      "Epoch 724 | Loss: -15.9602\n",
      "Epoch 725 | Loss: -15.9601\n",
      "Epoch 726 | Loss: -15.9601\n",
      "Epoch 727 | Loss: -15.9601\n",
      "Epoch 728 | Loss: -15.9602\n",
      "Epoch 729 | Loss: -15.9602\n",
      "Epoch 730 | Loss: -15.9602\n",
      "Epoch 731 | Loss: -15.9602\n",
      "Epoch 732 | Loss: -15.9602\n",
      "Epoch 733 | Loss: -15.9602\n",
      "Epoch 734 | Loss: -15.9601\n",
      "Epoch 735 | Loss: -15.9602\n",
      "Epoch 736 | Loss: -15.9602\n",
      "Epoch 737 | Loss: -15.9602\n",
      "Epoch 738 | Loss: -15.9602\n",
      "Epoch 739 | Loss: -15.9602\n",
      "Epoch 740 | Loss: -15.9602\n",
      "Epoch 741 | Loss: -15.9602\n",
      "Epoch 742 | Loss: -15.9602\n",
      "Epoch 743 | Loss: -15.9602\n",
      "Epoch 744 | Loss: -15.9602\n",
      "Epoch 745 | Loss: -15.9602\n",
      "Epoch 746 | Loss: -15.9602\n",
      "Epoch 747 | Loss: -15.9602\n",
      "Epoch 748 | Loss: -15.9602\n",
      "Epoch 749 | Loss: -15.9602\n",
      "Epoch 750 | Loss: -15.9602\n",
      "Epoch 751 | Loss: -15.9602\n",
      "Epoch 752 | Loss: -15.9602\n",
      "Epoch 753 | Loss: -15.9602\n",
      "Epoch 754 | Loss: -15.9602\n",
      "Epoch 755 | Loss: -15.9602\n",
      "Epoch 756 | Loss: -15.9602\n",
      "Epoch 757 | Loss: -15.9602\n",
      "Epoch 758 | Loss: -15.9602\n",
      "Epoch 759 | Loss: -15.9602\n",
      "Epoch 760 | Loss: -15.9602\n",
      "Epoch 761 | Loss: -15.9602\n",
      "Epoch 762 | Loss: -15.9602\n",
      "Epoch 763 | Loss: -15.9602\n",
      "Epoch 764 | Loss: -15.9602\n",
      "Epoch 765 | Loss: -15.9602\n",
      "Epoch 766 | Loss: -15.9602\n",
      "Epoch 767 | Loss: -15.9602\n",
      "Epoch 768 | Loss: -15.9602\n",
      "Epoch 769 | Loss: -15.9602\n",
      "Epoch 770 | Loss: -15.9602\n",
      "Epoch 771 | Loss: -15.9602\n",
      "Epoch 772 | Loss: -15.9602\n",
      "Epoch 773 | Loss: -15.9602\n",
      "Epoch 774 | Loss: -15.9602\n",
      "Epoch 775 | Loss: -15.9602\n",
      "Epoch 776 | Loss: -15.9602\n",
      "Epoch 777 | Loss: -15.9602\n",
      "Epoch 778 | Loss: -15.9602\n",
      "Epoch 779 | Loss: -15.9602\n",
      "Epoch 780 | Loss: -15.9602\n",
      "Epoch 781 | Loss: -15.9602\n",
      "Epoch 782 | Loss: -15.9602\n",
      "Epoch 783 | Loss: -15.9602\n",
      "Epoch 784 | Loss: -15.9603\n",
      "Epoch 785 | Loss: -15.9602\n",
      "Epoch 786 | Loss: -15.9602\n",
      "Epoch 787 | Loss: -15.9602\n",
      "Epoch 788 | Loss: -15.9602\n",
      "Epoch 789 | Loss: -15.9602\n",
      "Epoch 790 | Loss: -15.9602\n",
      "Epoch 791 | Loss: -15.9602\n",
      "Epoch 792 | Loss: -15.9602\n",
      "Epoch 793 | Loss: -15.9601\n",
      "Epoch 794 | Loss: -15.9601\n",
      "Epoch 795 | Loss: -15.9601\n",
      "Epoch 796 | Loss: -15.9600\n",
      "Epoch 797 | Loss: -15.9600\n",
      "Epoch 798 | Loss: -15.9601\n",
      "Epoch 799 | Loss: -15.9602\n",
      "Epoch 800 | Loss: -15.9602\n",
      "Epoch 801 | Loss: -15.9602\n",
      "Epoch 802 | Loss: -15.9602\n",
      "Epoch 803 | Loss: -15.9602\n",
      "Epoch 804 | Loss: -15.9602\n",
      "Epoch 805 | Loss: -15.9602\n",
      "Epoch 806 | Loss: -15.9602\n",
      "Epoch 807 | Loss: -15.9602\n",
      "Epoch 808 | Loss: -15.9602\n",
      "Epoch 809 | Loss: -15.9602\n",
      "Epoch 810 | Loss: -15.9602\n",
      "Epoch 811 | Loss: -15.9602\n",
      "Epoch 812 | Loss: -15.9602\n",
      "Epoch 813 | Loss: -15.9602\n",
      "Epoch 814 | Loss: -15.9602\n",
      "Epoch 815 | Loss: -15.9602\n",
      "Epoch 816 | Loss: -15.9602\n",
      "Epoch 817 | Loss: -15.9602\n",
      "Epoch 818 | Loss: -15.9602\n",
      "Epoch 819 | Loss: -15.9602\n",
      "Epoch 820 | Loss: -15.9602\n",
      "Epoch 821 | Loss: -15.9602\n",
      "Epoch 822 | Loss: -15.9602\n",
      "Epoch 823 | Loss: -15.9602\n",
      "Epoch 824 | Loss: -15.9602\n",
      "Epoch 825 | Loss: -15.9602\n",
      "Epoch 826 | Loss: -15.9602\n",
      "Epoch 827 | Loss: -15.9602\n",
      "Epoch 828 | Loss: -15.9602\n",
      "Epoch 829 | Loss: -15.9602\n",
      "Epoch 830 | Loss: -15.9602\n",
      "Epoch 831 | Loss: -15.9602\n",
      "Epoch 832 | Loss: -15.9602\n",
      "Epoch 833 | Loss: -15.9602\n",
      "Epoch 834 | Loss: -15.9602\n",
      "Epoch 835 | Loss: -15.9602\n",
      "Epoch 836 | Loss: -15.9603\n",
      "Epoch 837 | Loss: -15.9602\n",
      "Epoch 838 | Loss: -15.9602\n",
      "Epoch 839 | Loss: -15.9602\n",
      "Epoch 840 | Loss: -15.9602\n",
      "Epoch 841 | Loss: -15.9602\n",
      "Epoch 842 | Loss: -15.9602\n",
      "Epoch 843 | Loss: -15.9602\n",
      "Epoch 844 | Loss: -15.9602\n",
      "Epoch 845 | Loss: -15.9602\n",
      "Epoch 846 | Loss: -15.9602\n",
      "Epoch 847 | Loss: -15.9602\n",
      "Epoch 848 | Loss: -15.9602\n",
      "Epoch 849 | Loss: -15.9602\n",
      "Epoch 850 | Loss: -15.9602\n",
      "Epoch 851 | Loss: -15.9602\n",
      "Epoch 852 | Loss: -15.9602\n",
      "Epoch 853 | Loss: -15.9602\n",
      "Epoch 854 | Loss: -15.9602\n",
      "Epoch 855 | Loss: -15.9603\n",
      "Epoch 856 | Loss: -15.9602\n",
      "Epoch 857 | Loss: -15.9602\n",
      "Epoch 858 | Loss: -15.9602\n",
      "Epoch 859 | Loss: -15.9602\n",
      "Epoch 860 | Loss: -15.9602\n",
      "Epoch 861 | Loss: -15.9603\n",
      "Epoch 862 | Loss: -15.9602\n",
      "Epoch 863 | Loss: -15.9602\n",
      "Epoch 864 | Loss: -15.9602\n",
      "Epoch 865 | Loss: -15.9602\n",
      "Epoch 866 | Loss: -15.9603\n",
      "Epoch 867 | Loss: -15.9602\n",
      "Epoch 868 | Loss: -15.9602\n",
      "Epoch 869 | Loss: -15.9603\n",
      "Epoch 870 | Loss: -15.9602\n",
      "Epoch 871 | Loss: -15.9602\n",
      "Epoch 872 | Loss: -15.9602\n",
      "Epoch 873 | Loss: -15.9602\n",
      "Epoch 874 | Loss: -15.9602\n",
      "Epoch 875 | Loss: -15.9602\n",
      "Epoch 876 | Loss: -15.9602\n",
      "Epoch 877 | Loss: -15.9602\n",
      "Epoch 878 | Loss: -15.9602\n",
      "Epoch 879 | Loss: -15.9602\n",
      "Epoch 880 | Loss: -15.9603\n",
      "Epoch 881 | Loss: -15.9602\n",
      "Epoch 882 | Loss: -15.9602\n",
      "Epoch 883 | Loss: -15.9602\n",
      "Epoch 884 | Loss: -15.9603\n",
      "Epoch 885 | Loss: -15.9602\n",
      "Epoch 886 | Loss: -15.9602\n",
      "Epoch 887 | Loss: -15.9603\n",
      "Epoch 888 | Loss: -15.9602\n",
      "Epoch 889 | Loss: -15.9603\n",
      "Epoch 890 | Loss: -15.9603\n",
      "Epoch 891 | Loss: -15.9602\n",
      "Epoch 892 | Loss: -15.9602\n",
      "Epoch 893 | Loss: -15.9602\n",
      "Epoch 894 | Loss: -15.9602\n",
      "Epoch 895 | Loss: -15.9602\n",
      "Epoch 896 | Loss: -15.9602\n",
      "Epoch 897 | Loss: -15.9602\n",
      "Epoch 898 | Loss: -15.9602\n",
      "Epoch 899 | Loss: -15.9602\n",
      "Epoch 900 | Loss: -15.9602\n",
      "Epoch 901 | Loss: -15.9602\n",
      "Epoch 902 | Loss: -15.9602\n",
      "Epoch 903 | Loss: -15.9602\n",
      "Epoch 904 | Loss: -15.9602\n",
      "Epoch 905 | Loss: -15.9602\n",
      "Epoch 906 | Loss: -15.9602\n",
      "Epoch 907 | Loss: -15.9602\n",
      "Epoch 908 | Loss: -15.9602\n",
      "Epoch 909 | Loss: -15.9602\n",
      "Epoch 910 | Loss: -15.9601\n",
      "Epoch 911 | Loss: -15.9601\n",
      "Epoch 912 | Loss: -15.9600\n",
      "Epoch 913 | Loss: -15.9600\n",
      "Epoch 914 | Loss: -15.9599\n",
      "Epoch 915 | Loss: -15.9600\n",
      "Epoch 916 | Loss: -15.9601\n",
      "Epoch 917 | Loss: -15.9602\n",
      "Epoch 918 | Loss: -15.9602\n",
      "Epoch 919 | Loss: -15.9602\n",
      "Epoch 920 | Loss: -15.9601\n",
      "Epoch 921 | Loss: -15.9601\n",
      "Epoch 922 | Loss: -15.9602\n",
      "Epoch 923 | Loss: -15.9602\n",
      "Epoch 924 | Loss: -15.9602\n",
      "Epoch 925 | Loss: -15.9602\n",
      "Epoch 926 | Loss: -15.9602\n",
      "Epoch 927 | Loss: -15.9602\n",
      "Epoch 928 | Loss: -15.9602\n",
      "Epoch 929 | Loss: -15.9602\n",
      "Epoch 930 | Loss: -15.9602\n",
      "Epoch 931 | Loss: -15.9602\n",
      "Epoch 932 | Loss: -15.9602\n",
      "Epoch 933 | Loss: -15.9602\n",
      "Epoch 934 | Loss: -15.9602\n",
      "Epoch 935 | Loss: -15.9602\n",
      "Epoch 936 | Loss: -15.9602\n",
      "Epoch 937 | Loss: -15.9602\n",
      "Epoch 938 | Loss: -15.9602\n",
      "Epoch 939 | Loss: -15.9602\n",
      "Epoch 940 | Loss: -15.9603\n",
      "Epoch 941 | Loss: -15.9602\n",
      "Epoch 942 | Loss: -15.9602\n",
      "Epoch 943 | Loss: -15.9602\n",
      "Epoch 944 | Loss: -15.9602\n",
      "Epoch 945 | Loss: -15.9602\n",
      "Epoch 946 | Loss: -15.9602\n",
      "Epoch 947 | Loss: -15.9602\n",
      "Epoch 948 | Loss: -15.9602\n",
      "Epoch 949 | Loss: -15.9602\n",
      "Epoch 950 | Loss: -15.9602\n",
      "Epoch 951 | Loss: -15.9602\n",
      "Epoch 952 | Loss: -15.9603\n",
      "Epoch 953 | Loss: -15.9602\n",
      "Epoch 954 | Loss: -15.9602\n",
      "Epoch 955 | Loss: -15.9603\n",
      "Epoch 956 | Loss: -15.9602\n",
      "Epoch 957 | Loss: -15.9603\n",
      "Epoch 958 | Loss: -15.9602\n",
      "Epoch 959 | Loss: -15.9602\n",
      "Epoch 960 | Loss: -15.9602\n",
      "Epoch 961 | Loss: -15.9602\n",
      "Epoch 962 | Loss: -15.9602\n",
      "Epoch 963 | Loss: -15.9602\n",
      "Epoch 964 | Loss: -15.9602\n",
      "Epoch 965 | Loss: -15.9602\n",
      "Epoch 966 | Loss: -15.9602\n",
      "Epoch 967 | Loss: -15.9603\n",
      "Epoch 968 | Loss: -15.9602\n",
      "Epoch 969 | Loss: -15.9602\n",
      "Epoch 970 | Loss: -15.9602\n",
      "Epoch 971 | Loss: -15.9602\n",
      "Epoch 972 | Loss: -15.9602\n",
      "Epoch 973 | Loss: -15.9602\n",
      "Epoch 974 | Loss: -15.9602\n",
      "Epoch 975 | Loss: -15.9602\n",
      "Epoch 976 | Loss: -15.9602\n",
      "Epoch 977 | Loss: -15.9603\n",
      "Epoch 978 | Loss: -15.9602\n",
      "Epoch 979 | Loss: -15.9602\n",
      "Epoch 980 | Loss: -15.9602\n",
      "Epoch 981 | Loss: -15.9602\n",
      "Epoch 982 | Loss: -15.9602\n",
      "Epoch 983 | Loss: -15.9602\n",
      "Epoch 984 | Loss: -15.9602\n",
      "Epoch 985 | Loss: -15.9602\n",
      "Epoch 986 | Loss: -15.9602\n",
      "Epoch 987 | Loss: -15.9602\n",
      "Epoch 988 | Loss: -15.9602\n",
      "Epoch 989 | Loss: -15.9603\n",
      "Epoch 990 | Loss: -15.9603\n",
      "Epoch 991 | Loss: -15.9602\n",
      "Epoch 992 | Loss: -15.9602\n",
      "Epoch 993 | Loss: -15.9602\n",
      "Epoch 994 | Loss: -15.9602\n",
      "Epoch 995 | Loss: -15.9602\n",
      "Epoch 996 | Loss: -15.9602\n",
      "Epoch 997 | Loss: -15.9602\n",
      "Epoch 998 | Loss: -15.9602\n",
      "Epoch 999 | Loss: -15.9602\n",
      "Epoch 1000 | Loss: -15.9602\n"
     ]
    }
   ],
   "source": [
    "set_up = setup(3, 2, 2, 2, 100)\n",
    "# H_l = [torch.randn(3, 3, 2, 2, dtype=torch.cfloat) for _ in range(1)]\n",
    "CHCNN = ChannelCNN(set_up)\n",
    "tr = ChannelCNNTrainer(CHCNN, set_up, lr=1e-2)\n",
    "tr.train(train_list=H_l, dual_link_fn=alg_dual_link_torch, num_epochs=1000, batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-Unfolding-NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
